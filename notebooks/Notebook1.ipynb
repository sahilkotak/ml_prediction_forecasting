{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75b9ec06-f260-4ead-aa11-bd4b26492d6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=14X7usabTceBqo5d1-pcR9m5bbjoUPIzR\n",
      "To: E:\\UTS\\Sem2\\ADVMLA\\Assignment2\\Repo\\at2_assignment\\ml_pred_forecasting\\notebooks\\temp.csv\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 46.2k/46.2k [00:00<00:00, 458kB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1MsX7XJAAj-3whCKV9ZNAwO_h3hGwpx3q\n",
      "To: E:\\UTS\\Sem2\\ADVMLA\\Assignment2\\Repo\\at2_assignment\\ml_pred_forecasting\\notebooks\\temp.csv\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5.50k/5.50k [00:00<?, ?B/s]\n",
      "Downloading...\n",
      "From (uriginal): https://drive.google.com/uc?id=1Sxtwiy9PdGq_xxxVAPAWu7M_XqCKh0Lr\n",
      "From (redirected): https://drive.google.com/uc?id=1Sxtwiy9PdGq_xxxVAPAWu7M_XqCKh0Lr&confirm=t&uuid=284fb624-5bc7-44b9-8142-e7418847a8f3\n",
      "To: E:\\UTS\\Sem2\\ADVMLA\\Assignment2\\Repo\\at2_assignment\\ml_pred_forecasting\\notebooks\\temp.csv\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 203M/203M [00:34<00:00, 5.98MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1Zu_bRhPg0CnHW-ihAHurzWTk7uJ11uRC\n",
      "To: E:\\UTS\\Sem2\\ADVMLA\\Assignment2\\Repo\\at2_assignment\\ml_pred_forecasting\\notebooks\\temp.csv\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 24.7M/24.7M [00:06<00:00, 3.80MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1kSciVM9LLaLsJXiY1vaAO-UtBzZ-YE6l\n",
      "To: E:\\UTS\\Sem2\\ADVMLA\\Assignment2\\Repo\\at2_assignment\\ml_pred_forecasting\\notebooks\\temp.csv\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 97.1M/97.1M [00:15<00:00, 6.39MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         date  wm_yr_wk    d\n",
      "0  2011-01-29     11101  d_1\n",
      "1  2011-01-30     11101  d_2\n",
      "2  2011-01-31     11101  d_3\n",
      "3  2011-02-01     11101  d_4\n",
      "4  2011-02-02     11101  d_5\n",
      "         date     event_name event_type\n",
      "0  2011-02-06      SuperBowl   Sporting\n",
      "1  2011-02-14  ValentinesDay   Cultural\n",
      "2  2011-02-21  PresidentsDay   National\n",
      "3  2011-03-09      LentStart  Religious\n",
      "4  2011-03-16      LentWeek2  Religious\n",
      "  store_id        item_id  wm_yr_wk  sell_price\n",
      "0     CA_1  HOBBIES_1_001     11325        9.58\n",
      "1     CA_1  HOBBIES_1_001     11326        9.58\n",
      "2     CA_1  HOBBIES_1_001     11327        8.26\n",
      "3     CA_1  HOBBIES_1_001     11328        8.26\n",
      "4     CA_1  HOBBIES_1_001     11329        8.26\n",
      "   d_1542  d_1543  d_1544  d_1545  d_1546  d_1547  d_1548  d_1549  d_1550   \n",
      "0       0       1       0       2       1       0       2       0       1  \\\n",
      "1       0       0       0       0       0       0       0       0       1   \n",
      "2       0       0       0       0       0       1       0       0       0   \n",
      "3       4       1       0       1       3       5       2       3       0   \n",
      "4       3       0       0       1       1       0       2       0       2   \n",
      "\n",
      "   d_1551  ...  d_1932  d_1933  d_1934  d_1935  d_1936  d_1937  d_1938   \n",
      "0       0  ...       2       4       0       0       0       0       3  \\\n",
      "1       0  ...       0       1       2       1       1       0       0   \n",
      "2       0  ...       1       0       2       0       0       0       2   \n",
      "3       2  ...       1       1       0       4       0       1       3   \n",
      "4       1  ...       0       0       0       2       1       0       0   \n",
      "\n",
      "   d_1939  d_1940  d_1941  \n",
      "0       3       0       1  \n",
      "1       0       0       0  \n",
      "2       3       0       1  \n",
      "3       0       2       6  \n",
      "4       2       1       0  \n",
      "\n",
      "[5 rows x 400 columns]\n",
      "                              id        item_id    dept_id   cat_id store_id   \n",
      "0  HOBBIES_1_001_CA_1_evaluation  HOBBIES_1_001  HOBBIES_1  HOBBIES     CA_1  \\\n",
      "1  HOBBIES_1_002_CA_1_evaluation  HOBBIES_1_002  HOBBIES_1  HOBBIES     CA_1   \n",
      "2  HOBBIES_1_003_CA_1_evaluation  HOBBIES_1_003  HOBBIES_1  HOBBIES     CA_1   \n",
      "3  HOBBIES_1_004_CA_1_evaluation  HOBBIES_1_004  HOBBIES_1  HOBBIES     CA_1   \n",
      "4  HOBBIES_1_005_CA_1_evaluation  HOBBIES_1_005  HOBBIES_1  HOBBIES     CA_1   \n",
      "\n",
      "  state_id  d_1  d_2  d_3  d_4  ...  d_1532  d_1533  d_1534  d_1535  d_1536   \n",
      "0       CA    0    0    0    0  ...       1       1       1       0       1  \\\n",
      "1       CA    0    0    0    0  ...       0       0       0       0       0   \n",
      "2       CA    0    0    0    0  ...       0       0       1       0       0   \n",
      "3       CA    0    0    0    0  ...       8       2       0       8       2   \n",
      "4       CA    0    0    0    0  ...       2       0       1       3       2   \n",
      "\n",
      "   d_1537  d_1538  d_1539  d_1540  d_1541  \n",
      "0       0       1       0       0       1  \n",
      "1       0       0       0       1       0  \n",
      "2       0       0       0       0       0  \n",
      "3       3       1       1       3       8  \n",
      "4       1       1       2       2       3  \n",
      "\n",
      "[5 rows x 1547 columns]\n"
     ]
    }
   ],
   "source": [
    "import gdown\n",
    "import pandas as pd\n",
    "\n",
    "# URLs for the datasets\n",
    "urls = [\n",
    "    \"https://drive.google.com/uc?id=14X7usabTceBqo5d1-pcR9m5bbjoUPIzR\",\n",
    "    \"https://drive.google.com/uc?id=1MsX7XJAAj-3whCKV9ZNAwO_h3hGwpx3q\",\n",
    "    \"https://drive.google.com/uc?id=1Sxtwiy9PdGq_xxxVAPAWu7M_XqCKh0Lr\",\n",
    "    \"https://drive.google.com/uc?id=1Zu_bRhPg0CnHW-ihAHurzWTk7uJ11uRC\",\n",
    "    \"https://drive.google.com/uc?id=1kSciVM9LLaLsJXiY1vaAO-UtBzZ-YE6l\"\n",
    "]\n",
    "\n",
    "dfs = []  # list to hold the dataframes\n",
    "\n",
    "# Download and read each dataset into a dataframe\n",
    "for url in urls:\n",
    "    gdown.download(url, 'temp.csv', quiet=False)\n",
    "    df = pd.read_csv('temp.csv')\n",
    "    dfs.append(df)\n",
    "\n",
    "# Assign each dataframe to a variable\n",
    "df1, df2, df3, df4, df5 = dfs\n",
    "\n",
    "# View the first few rows of each dataframe to understand their structure\n",
    "print(df1.head())\n",
    "print(df2.head())\n",
    "print(df3.head())\n",
    "print(df4.head())\n",
    "print(df5.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd6760c9-02c6-4833-a401-ef82a0c98fe1",
   "metadata": {},
   "source": [
    "From the output above, it appears that we have the following data structures:\n",
    "\n",
    "- Calendar Data (df1): Contains the date, the corresponding week identifier, and a day identifier.\n",
    "- Events Data (df2): Contains the date, event name, and event type.\n",
    "- Item Price Data (df3): Contains the store ID, item ID, week identifier, and sell price.\n",
    "- Evaluation Data (df4): Appears to contain sales data for multiple days, but the structure isn't clear just from the first few rows. Each column d_xxx seems to represent the sales for a specific day.\n",
    "- Training Data (df5): Contains an ID, item ID, department ID, category ID, store ID, state ID, and sales data for multiple days.\n",
    "\n",
    "Observations & Proposed Changes:\n",
    "\n",
    "- The Calendar Data looks fine.\n",
    "- The Events Data seems appropriate as well.\n",
    "- Item Price Data is in good shape, containing the required details.\n",
    "\n",
    "- Evaluation Data (df4):\n",
    "  - This seems to be in wide format with sales data for each day represented as a separate column. It might be helpful to melt this dataframe to a long format for easier analysis and modeling.\n",
    "\n",
    "\n",
    "- Training Data (df5):\n",
    "  - This dataframe is also in a wide format. It might be beneficial to melt this into a long format similar to the evaluation data.\n",
    "\n",
    "To transform the wide dataframe into a long format, we'll use the melt function from pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e738764c-3037-4338-a7f6-d7a9debaccea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              id        item_id    dept_id   cat_id store_id   \n",
      "0  HOBBIES_1_001_CA_1_evaluation  HOBBIES_1_001  HOBBIES_1  HOBBIES     CA_1  \\\n",
      "1  HOBBIES_1_002_CA_1_evaluation  HOBBIES_1_002  HOBBIES_1  HOBBIES     CA_1   \n",
      "2  HOBBIES_1_003_CA_1_evaluation  HOBBIES_1_003  HOBBIES_1  HOBBIES     CA_1   \n",
      "3  HOBBIES_1_004_CA_1_evaluation  HOBBIES_1_004  HOBBIES_1  HOBBIES     CA_1   \n",
      "4  HOBBIES_1_005_CA_1_evaluation  HOBBIES_1_005  HOBBIES_1  HOBBIES     CA_1   \n",
      "\n",
      "  state_id    d  sales  \n",
      "0       CA  d_1      0  \n",
      "1       CA  d_1      0  \n",
      "2       CA  d_1      0  \n",
      "3       CA  d_1      0  \n",
      "4       CA  d_1      0  \n"
     ]
    }
   ],
   "source": [
    "# Melt the training data\n",
    "df5_melted = pd.melt(df5, \n",
    "                     id_vars=['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'], \n",
    "                     var_name='d', \n",
    "                     value_name='sales')\n",
    "\n",
    "# Display the first few rows of the transformed dataframe\n",
    "print(df5_melted.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46de1cc2-d78e-43e5-a069-a8365413c5ae",
   "metadata": {},
   "source": [
    "To verify the transformation and ensure correctness, we will perform the following checks:\n",
    "\n",
    "- Dimensionality Check: Ensure that the total number of rows in the melted dataframes matches the expected count based on the original dataframes.\n",
    "    - For df5, the number of rows in the melted version should be: \"number of unique items × number of days in the dataset\"\n",
    "\n",
    "- Value Check: Sample a few item-day combinations and compare the sales values in the melted dataframe with the corresponding values in the wide dataframe. This ensures that the melt operation hasn't inadvertently changed any values.\n",
    "\n",
    "- Missing Values: Ensure that there are no missing values in the sales column of the melted dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a11c8e8c-7a8f-4a2a-9333-68892f27edc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All checks passed!\n"
     ]
    }
   ],
   "source": [
    "# Dimensionality Check\n",
    "assert len(df5_melted) == len(df5) * len(df5.columns[6:]), \"Mismatch in the number of rows for df5_melted\"\n",
    "\n",
    "# Value Check for df5\n",
    "# Sample a random item-day combination\n",
    "sample_item = df5_melted['item_id'].sample().iloc[0]\n",
    "sample_day = df5_melted['d'].sample().iloc[0]\n",
    "assert df5_melted[(df5_melted['item_id'] == sample_item) & (df5_melted['d'] == sample_day)]['sales'].iloc[0] == df5[df5['item_id'] == sample_item][sample_day].iloc[0], \"Mismatch in sales value for df5_melted\"\n",
    "\n",
    "# Missing Values Check\n",
    "assert df5_melted['sales'].isna().sum() == 0, \"Missing values detected in df5_melted['sales']\"\n",
    "\n",
    "print(\"All checks passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26709070-1eac-4cdb-b1c0-bb9fbb140547",
   "metadata": {},
   "source": [
    "### Joining Dataframes:\n",
    "- First, we'll join the training data (df5_melted) with the calendar data (df1) on the 'd' column. This will give us date-related information for each sales entry. Then, we'll join this resulting dataframe with the item price data (df3) on 'item_id', 'store_id', and 'wm_yr_wk' to incorporate the selling price for each item on each day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ee88df55-6dff-4301-827b-24d926a3116d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              id        item_id    dept_id   cat_id store_id   \n",
      "0  HOBBIES_1_001_CA_1_evaluation  HOBBIES_1_001  HOBBIES_1  HOBBIES     CA_1  \\\n",
      "1  HOBBIES_1_002_CA_1_evaluation  HOBBIES_1_002  HOBBIES_1  HOBBIES     CA_1   \n",
      "2  HOBBIES_1_003_CA_1_evaluation  HOBBIES_1_003  HOBBIES_1  HOBBIES     CA_1   \n",
      "3  HOBBIES_1_004_CA_1_evaluation  HOBBIES_1_004  HOBBIES_1  HOBBIES     CA_1   \n",
      "4  HOBBIES_1_005_CA_1_evaluation  HOBBIES_1_005  HOBBIES_1  HOBBIES     CA_1   \n",
      "\n",
      "  state_id    d  sales        date  wm_yr_wk  \n",
      "0       CA  d_1      0  2011-01-29     11101  \n",
      "1       CA  d_1      0  2011-01-29     11101  \n",
      "2       CA  d_1      0  2011-01-29     11101  \n",
      "3       CA  d_1      0  2011-01-29     11101  \n",
      "4       CA  d_1      0  2011-01-29     11101  \n"
     ]
    }
   ],
   "source": [
    "# Filter df1 to only include rows up to d_1541\n",
    "df1_filtered = df1[df1['d'].isin(df5_melted['d'])]\n",
    "\n",
    "# Merge the dataframes\n",
    "merged_data = pd.merge(df5_melted, df1_filtered, on='d', how='left')\n",
    "\n",
    "# Display the first few rows of the merged dataframe\n",
    "print(merged_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "333d5e59-d87b-40c0-bcab-6fec86041840",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              id        item_id    dept_id   cat_id store_id   \n",
      "0  HOBBIES_1_001_CA_1_evaluation  HOBBIES_1_001  HOBBIES_1  HOBBIES     CA_1  \\\n",
      "1  HOBBIES_1_002_CA_1_evaluation  HOBBIES_1_002  HOBBIES_1  HOBBIES     CA_1   \n",
      "2  HOBBIES_1_003_CA_1_evaluation  HOBBIES_1_003  HOBBIES_1  HOBBIES     CA_1   \n",
      "3  HOBBIES_1_004_CA_1_evaluation  HOBBIES_1_004  HOBBIES_1  HOBBIES     CA_1   \n",
      "4  HOBBIES_1_005_CA_1_evaluation  HOBBIES_1_005  HOBBIES_1  HOBBIES     CA_1   \n",
      "\n",
      "  state_id    d  sales        date  wm_yr_wk  sell_price  \n",
      "0       CA  d_1      0  2011-01-29     11101         NaN  \n",
      "1       CA  d_1      0  2011-01-29     11101         NaN  \n",
      "2       CA  d_1      0  2011-01-29     11101         NaN  \n",
      "3       CA  d_1      0  2011-01-29     11101         NaN  \n",
      "4       CA  d_1      0  2011-01-29     11101         NaN  \n"
     ]
    }
   ],
   "source": [
    "merged_train = pd.merge(merged_data, df3, on=['store_id', 'item_id', 'wm_yr_wk'], how='left')\n",
    "print(merged_train.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ec840a-41f4-47cc-982a-960f84937925",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a029bf5-8198-4600-ab2b-c6ba882940b5",
   "metadata": {},
   "source": [
    "### Step 1: Handle Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ee61ed14-0650-4544-99e5-9ed95037ac92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [store_id, item_id, wm_yr_wk, sell_price]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# Sample missing data\n",
    "sample_missing = merged_train[merged_train['sell_price'].isna()].sample()\n",
    "sample_item = sample_missing['item_id'].iloc[0]\n",
    "sample_store = sample_missing['store_id'].iloc[0]\n",
    "sample_week = sample_missing['wm_yr_wk'].iloc[0]\n",
    "\n",
    "# Check in original price data\n",
    "price_check = df3[(df3['item_id'] == sample_item) & (df3['store_id'] == sample_store) & (df3['wm_yr_wk'] == sample_week)]\n",
    "print(price_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f00c20a3-ed3d-4c59-bc58-609b8ef97db1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weeks with missing prices for HOUSEHOLD_1_159: \n",
      "[11101, 11102, 11103, 11104, 11105, 11106, 11107, 11108, 11109, 11110, 11111, 11112, 11113, 11114, 11115, 11116, 11117, 11118, 11119, 11120, 11121, 11122, 11123, 11124, 11125, 11126, 11127, 11128, 11129, 11130, 11131, 11132, 11133, 11134, 11135, 11136, 11137, 11138, 11139, 11140, 11141, 11142, 11143, 11144, 11145, 11146, 11147, 11148, 11149, 11150, 11151, 11152, 11201, 11202, 11203, 11204, 11205, 11206, 11207, 11208, 11209, 11210, 11211, 11212, 11213, 11214, 11215, 11216, 11217, 11218, 11219, 11220, 11221, 11222, 11223, 11224, 11225, 11226, 11227, 11228, 11229, 11230, 11231, 11232, 11233, 11234, 11235, 11236, 11237, 11238, 11239, 11240, 11241, 11242, 11243, 11244, 11245, 11246, 11247, 11248, 11249, 11250, 11251, 11252, 11301, 11302, 11303, 11304, 11305, 11306, 11307, 11308, 11309, 11310, 11311, 11312, 11313, 11314, 11315, 11316, 11317, 11318, 11319, 11320, 11321, 11322, 11323, 11324, 11325, 11326, 11327, 11328, 11329, 11330, 11331, 11332, 11333, 11334, 11335, 11336, 11337, 11338, 11339, 11340, 11341, 11342, 11343, 11344, 11345, 11346, 11347, 11348, 11349, 11350, 11351, 11352, 11353, 11401, 11402, 11403, 11404, 11405, 11406, 11407, 11408, 11409, 11410, 11411, 11412, 11413, 11414, 11415, 11416, 11417, 11418, 11419, 11420, 11421, 11422, 11423, 11424, 11425, 11426, 11427, 11428, 11429, 11430, 11431, 11432, 11433, 11434, 11435, 11436, 11437, 11438, 11439, 11440, 11441, 11442, 11443, 11444, 11445, 11446, 11447, 11448, 11449, 11450, 11451, 11452, 11501, 11502, 11503, 11504, 11505, 11506, 11507, 11508, 11509, 11510, 11511, 11512]\n"
     ]
    }
   ],
   "source": [
    "missing_weeks_item = merged_train[(merged_train['item_id'] == 'HOUSEHOLD_1_159') & (merged_train['sell_price'].isna())]['wm_yr_wk'].unique()\n",
    "print(f\"Weeks with missing prices for HOUSEHOLD_1_159: \\n{sorted(missing_weeks_item)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8765c197-41cf-4784-8fbe-39484d80bbd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of combinations in merged data not present in price data: 1752423\n"
     ]
    }
   ],
   "source": [
    "# Unique combinations in the merged dataframe\n",
    "merged_combinations = set(merged_train[['item_id', 'store_id', 'wm_yr_wk']].itertuples(index=False))\n",
    "\n",
    "# Unique combinations in the price dataframe\n",
    "price_combinations = set(df3[['item_id', 'store_id', 'wm_yr_wk']].itertuples(index=False))\n",
    "\n",
    "# Find combinations in the merged dataframe that are not in the price dataframe\n",
    "missing_combinations = merged_combinations - price_combinations\n",
    "\n",
    "print(f\"Number of combinations in merged data not present in price data: {len(missing_combinations)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bdbae9bf-ce1d-475c-917e-0c116deeca19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample missing combinations:\n",
      " [Pandas(item_id='HOBBIES_2_021', store_id='CA_1', wm_yr_wk=11231), Pandas(item_id='FOODS_3_821', store_id='WI_1', wm_yr_wk=11319), Pandas(item_id='FOODS_1_025', store_id='CA_1', wm_yr_wk=11135), Pandas(item_id='FOODS_3_737', store_id='WI_1', wm_yr_wk=11104), Pandas(item_id='FOODS_3_197', store_id='TX_3', wm_yr_wk=11228)]\n"
     ]
    }
   ],
   "source": [
    "sample_missing_combinations = list(missing_combinations)[:5]\n",
    "print(\"Sample missing combinations:\\n\", sample_missing_combinations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c07f47f2-8627-4984-b274-18ea934952c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data types in merged_train:\n",
      " item_id     object\n",
      "store_id    object\n",
      "wm_yr_wk     int64\n",
      "dtype: object\n",
      "\n",
      "Data types in df3:\n",
      " item_id     object\n",
      "store_id    object\n",
      "wm_yr_wk     int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(\"Data types in merged_train:\\n\", merged_train[['item_id', 'store_id', 'wm_yr_wk']].dtypes)\n",
    "print(\"\\nData types in df3:\\n\", df3[['item_id', 'store_id', 'wm_yr_wk']].dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "af0cddf5-0a6c-4360-bf5e-0a18a356892c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [store_id, item_id, wm_yr_wk, sell_price]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "duplicates = df3[df3.duplicated(subset=['store_id', 'item_id', 'wm_yr_wk'], keep=False)]\n",
    "print(duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7a4b34ef-795b-41f2-81cc-f289ea71333a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     store_id        item_id  wm_yr_wk  sell_price\n",
      "0        CA_1  HOBBIES_1_001     11325        9.58\n",
      "1        CA_1  HOBBIES_1_001     11326        9.58\n",
      "2        CA_1  HOBBIES_1_001     11327        8.26\n",
      "3        CA_1  HOBBIES_1_001     11328        8.26\n",
      "4        CA_1  HOBBIES_1_001     11329        8.26\n",
      "...       ...            ...       ...         ...\n",
      "2495     CA_1  HOBBIES_1_011     11445        3.48\n",
      "2496     CA_1  HOBBIES_1_011     11446        3.48\n",
      "2497     CA_1  HOBBIES_1_011     11447        3.48\n",
      "2498     CA_1  HOBBIES_1_011     11448        3.48\n",
      "2499     CA_1  HOBBIES_1_011     11449        3.48\n",
      "\n",
      "[2500 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df3.head(2500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dc599890-caa9-4574-9c4a-00af5fb0fae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [store_id, item_id, wm_yr_wk, sell_price]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "sample_check = df3[df3[['store_id', 'item_id', 'wm_yr_wk']].apply(tuple, axis=1).isin(sample_missing_combinations)]\n",
    "print(sample_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7d8d9ede-9317-4d7a-b25d-94e0d3a42b7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weeks in merged_data not in df3: set()\n",
      "Weeks in df3 not in merged_data: {11520, 11521, 11522, 11523, 11524, 11525, 11526, 11527, 11528, 11529, 11530, 11531, 11532, 11533, 11534, 11535, 11536, 11537, 11538, 11539, 11540, 11541, 11542, 11543, 11544, 11545, 11546, 11547, 11548, 11549, 11550, 11551, 11552, 11601, 11602, 11603, 11604, 11605, 11606, 11607, 11608, 11609, 11610, 11611, 11612, 11613, 11614, 11615, 11616, 11617, 11618, 11619, 11620, 11621, 11513, 11514, 11515, 11516, 11517, 11518, 11519}\n"
     ]
    }
   ],
   "source": [
    "missing_weeks_in_df3 = set(merged_data['wm_yr_wk'].unique()) - set(df3['wm_yr_wk'].unique())\n",
    "print(\"Weeks in merged_data not in df3:\", missing_weeks_in_df3)\n",
    "\n",
    "missing_weeks_in_merged_data = set(df3['wm_yr_wk'].unique()) - set(merged_data['wm_yr_wk'].unique())\n",
    "print(\"Weeks in df3 not in merged_data:\", missing_weeks_in_merged_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "25ed8d73-2239-4a63-be14-fceb4b41bb2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of combinations in merged_data not present in df3: 1752423\n",
      "Sample missing combinations: [Pandas(store_id='CA_1', item_id='HOBBIES_1_279', wm_yr_wk=11249), Pandas(store_id='WI_3', item_id='HOBBIES_1_171', wm_yr_wk=11124), Pandas(store_id='WI_3', item_id='FOODS_3_284', wm_yr_wk=11131), Pandas(store_id='TX_2', item_id='FOODS_3_060', wm_yr_wk=11102), Pandas(store_id='WI_3', item_id='HOBBIES_1_395', wm_yr_wk=11144)]\n"
     ]
    }
   ],
   "source": [
    "# Get unique combinations in both dataframes\n",
    "merged_data_combinations = set(merged_data[['store_id', 'item_id', 'wm_yr_wk']].itertuples(index=False))\n",
    "df3_combinations = set(df3[['store_id', 'item_id', 'wm_yr_wk']].itertuples(index=False))\n",
    "\n",
    "# Find combinations in merged_data that aren't in df3\n",
    "missing_combinations = merged_data_combinations - df3_combinations\n",
    "\n",
    "print(\"Number of combinations in merged_data not present in df3:\", len(missing_combinations))\n",
    "print(\"Sample missing combinations:\", list(missing_combinations)[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3db56c-2dfa-43bd-bf4b-0e2329ee302e",
   "metadata": {},
   "source": [
    "#### Check the Percentage of Missing Values:\n",
    "- Let's see how widespread the missing values are in the sell_price column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "bba49707-d6e3-404b-a943-88572eda956d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of missing values in sell_price: 26.10%\n"
     ]
    }
   ],
   "source": [
    "missing_percentage = (merged_train['sell_price'].isna().sum() / len(merged_train)) * 100\n",
    "print(f\"Percentage of missing values in sell_price: {missing_percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1cf5c739-a3ea-41b2-9b24-88beafe97b77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "store_id      0\n",
      "item_id       0\n",
      "wm_yr_wk      0\n",
      "sell_price    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "missing_values_df3 = df3.isnull().sum()\n",
    "print(missing_values_df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f3e57462-e13b-41fe-918d-e72d14166925",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Series([], Name: count, dtype: int64)\n"
     ]
    }
   ],
   "source": [
    "items_missing_prices = df3[df3['sell_price'].isnull()]['item_id'].value_counts()\n",
    "print(items_missing_prices.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "973fe897-603e-45d8-8106-c70baf402da4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Series([], Name: count, dtype: int64)\n"
     ]
    }
   ],
   "source": [
    "stores_missing_prices = df3[df3['sell_price'].isnull()]['store_id'].value_counts()\n",
    "print(stores_missing_prices.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ec04fb78-c108-4dc8-91ba-1414651a5291",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Series([], Name: count, dtype: int64)\n"
     ]
    }
   ],
   "source": [
    "weeks_missing_prices = df3[df3['sell_price'].isnull()]['wm_yr_wk'].value_counts()\n",
    "print(weeks_missing_prices.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8d4569d5-190e-4f16-9d46-05907b9dd181",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_d_sales = merged_train['d'].nunique()\n",
    "unique_d_calendar = df1['d'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "122d429b-2ef7-48e7-9f60-bc3c7a35258a",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_rows = merged_train.shape[0]\n",
    "unique_d_merged = merged_train['d'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "bc7a62b2-a7a1-47e2-9094-c64aa64c8221",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weeks with the most missing price data:\n",
      "wm_yr_wk\n",
      "11101    136906\n",
      "11102    129605\n",
      "11103    125797\n",
      "11104    123837\n",
      "11105    122199\n",
      "11106    120092\n",
      "11107    118223\n",
      "11108    116802\n",
      "11109    115759\n",
      "11110    114415\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Extract weeks with missing prices from the merged data\n",
    "missing_weeks = merged_train[merged_train['sell_price'].isnull()]['wm_yr_wk'].value_counts()\n",
    "\n",
    "# Display weeks with the most missing data\n",
    "print(\"Weeks with the most missing price data:\")\n",
    "print(missing_weeks.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "22caedbc-4a30-41ca-a997-935c9b7ccb89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Items with the most missing price data:\n",
      "item_id\n",
      "HOUSEHOLD_1_159    15297\n",
      "HOUSEHOLD_1_242    15188\n",
      "HOUSEHOLD_1_308    15150\n",
      "HOUSEHOLD_2_186    15009\n",
      "FOODS_3_353        14981\n",
      "HOUSEHOLD_1_489    14889\n",
      "HOUSEHOLD_1_534    14882\n",
      "HOUSEHOLD_1_484    14882\n",
      "FOODS_3_255        14854\n",
      "HOUSEHOLD_1_526    14812\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Extract items with missing prices from the merged data\n",
    "missing_items = merged_train[merged_train['sell_price'].isnull()]['item_id'].value_counts()\n",
    "\n",
    "# Display items with the most missing data\n",
    "print(\"Items with the most missing price data:\")\n",
    "print(missing_items.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "842059ab-66d0-4c4c-ab7f-8dfe0cb8703b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stores with the most missing price data:\n",
      "store_id\n",
      "CA_2    1540457\n",
      "WI_1    1355701\n",
      "WI_2    1269390\n",
      "CA_4    1261195\n",
      "TX_3    1178191\n",
      "CA_3    1158667\n",
      "WI_3    1145123\n",
      "CA_1    1127909\n",
      "TX_1    1118871\n",
      "TX_2    1108895\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Extract stores with missing prices from the merged data\n",
    "missing_stores = merged_train[merged_train['sell_price'].isnull()]['store_id'].value_counts()\n",
    "\n",
    "# Display stores with the most missing data\n",
    "print(\"Stores with the most missing price data:\")\n",
    "print(missing_stores.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b04f36b7-d9f0-4c0c-8bc5-af2ddd795c97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing weeks for item 'HOUSEHOLD_1_159' in store 'CA_2': 219\n",
      "Weeks with missing prices for HOUSEHOLD_1_159 in CA_2: \n",
      "[11101, 11102, 11103, 11104, 11105, 11106, 11107, 11108, 11109, 11110, 11111, 11112, 11113, 11114, 11115, 11116, 11117, 11118, 11119, 11120, 11121, 11122, 11123, 11124, 11125, 11126, 11127, 11128, 11129, 11130, 11131, 11132, 11133, 11134, 11135, 11136, 11137, 11138, 11139, 11140, 11141, 11142, 11143, 11144, 11145, 11146, 11147, 11148, 11149, 11150, 11151, 11152, 11201, 11202, 11203, 11204, 11205, 11206, 11207, 11208, 11209, 11210, 11211, 11212, 11213, 11214, 11215, 11216, 11217, 11218, 11219, 11220, 11221, 11222, 11223, 11224, 11225, 11226, 11227, 11228, 11229, 11230, 11231, 11232, 11233, 11234, 11235, 11236, 11237, 11238, 11239, 11240, 11241, 11242, 11243, 11244, 11245, 11246, 11247, 11248, 11249, 11250, 11251, 11252, 11301, 11302, 11303, 11304, 11305, 11306, 11307, 11308, 11309, 11310, 11311, 11312, 11313, 11314, 11315, 11316, 11317, 11318, 11319, 11320, 11321, 11322, 11323, 11324, 11325, 11326, 11327, 11328, 11329, 11330, 11331, 11332, 11333, 11334, 11335, 11336, 11337, 11338, 11339, 11340, 11341, 11342, 11343, 11344, 11345, 11346, 11347, 11348, 11349, 11350, 11351, 11352, 11353, 11401, 11402, 11403, 11404, 11405, 11406, 11407, 11408, 11409, 11410, 11411, 11412, 11413, 11414, 11415, 11416, 11417, 11418, 11419, 11420, 11421, 11422, 11423, 11424, 11425, 11426, 11427, 11428, 11429, 11430, 11431, 11432, 11433, 11434, 11435, 11436, 11437, 11438, 11439, 11440, 11441, 11442, 11443, 11444, 11445, 11446, 11447, 11448, 11449, 11450, 11451, 11452, 11501, 11502, 11503, 11504, 11505, 11506, 11507, 11508, 11509, 11510]\n"
     ]
    }
   ],
   "source": [
    "# Filtering data for the item 'HOUSEHOLD_1_159' in store 'CA_2' which has missing prices\n",
    "filtered_data = merged_train[(merged_train['item_id'] == 'HOUSEHOLD_1_159') & \n",
    "                            (merged_train['store_id'] == 'CA_2') & \n",
    "                            (merged_train['sell_price'].isnull())]\n",
    "\n",
    "# Extract weeks with missing prices for this combination\n",
    "missing_weeks_for_combination = filtered_data['wm_yr_wk'].unique()\n",
    "\n",
    "print(f\"Number of missing weeks for item 'HOUSEHOLD_1_159' in store 'CA_2': {len(missing_weeks_for_combination)}\")\n",
    "print(f\"Weeks with missing prices for HOUSEHOLD_1_159 in CA_2: \\n{sorted(missing_weeks_for_combination)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0329d7b6-4030-482c-b0bd-d5aae723f1fc",
   "metadata": {},
   "source": [
    "#### Investigate the Missing Values:\n",
    "- If the percentage of missing values is significant, we should investigate further. For example, check if specific items or stores have more missing prices than others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ac7b5c76-60e5-4dd2-a496-474c3b40b22c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of missing sell prices for 'HOUSEHOLD_1_159' in 'CA_2': 99.48%\n"
     ]
    }
   ],
   "source": [
    "subset_data = merged_train[(merged_train['item_id'] == 'HOUSEHOLD_1_159') & (merged_train['store_id'] == 'CA_2')]\n",
    "missing_percentage = (subset_data['sell_price'].isnull().sum() / len(subset_data)) * 100\n",
    "print(f\"Percentage of missing sell prices for 'HOUSEHOLD_1_159' in 'CA_2': {missing_percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "04f355b8-0893-4d6c-ab95-a66a86ea42c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              item_id store_id  sell_price\n",
      "0         FOODS_1_029     CA_4   97.663855\n",
      "1         FOODS_1_122     CA_1   90.850097\n",
      "2         FOODS_1_122     CA_2   90.850097\n",
      "3         FOODS_1_122     CA_3   90.850097\n",
      "4         FOODS_1_122     CA_4   90.850097\n",
      "...               ...      ...         ...\n",
      "1126  HOUSEHOLD_2_418     WI_2   92.667099\n",
      "1127  HOUSEHOLD_2_418     WI_3   93.121350\n",
      "1128  HOUSEHOLD_2_434     TX_3   92.667099\n",
      "1129  HOUSEHOLD_2_498     CA_4   92.667099\n",
      "1130  HOUSEHOLD_2_498     WI_3   93.575600\n",
      "\n",
      "[1131 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# Group by item_id and store_id\n",
    "grouped = merged_train.groupby(['item_id', 'store_id'])\n",
    "\n",
    "# Calculate the percentage of missing sell_price for each group\n",
    "missing_percentage = grouped['sell_price'].apply(lambda x: x.isnull().mean() * 100)\n",
    "\n",
    "# Filter out the combinations where the missing percentage is greater than 90%\n",
    "high_missing_combinations = missing_percentage[missing_percentage > 90].reset_index()\n",
    "\n",
    "print(high_missing_combinations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "2c62de77-1ba4-4562-849c-877c4eb2fc36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.709412922269597\n"
     ]
    }
   ],
   "source": [
    "# Calculate the total number of unique item-store combinations in the merged_data\n",
    "total_item_store_combinations = merged_train[['item_id', 'store_id']].drop_duplicates().shape[0]\n",
    "\n",
    "# Calculate the percentage of problematic item-store combinations\n",
    "percentage_problematic = (1131 / total_item_store_combinations) * 100\n",
    "\n",
    "print(percentage_problematic)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9defbd52-96e9-45b2-b6ea-06de04a85571",
   "metadata": {},
   "source": [
    "- Drop: If the missing data is random and constitutes a small portion of the dataset.\n",
    "- Impute: If the missing data follows a pattern or is a significant portion of the dataset.\n",
    "\n",
    "Action Plan:\n",
    "\n",
    "- Calculate the percentage of missing values for each feature.\n",
    "- Based on the percentage, decide whether to drop or impute.\n",
    "\n",
    "Let's first inspect the percentage of missing values in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "17f4ac8e-4cc7-42eb-a04b-95b7d5513234",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sell_price    26.102747\n",
       "dtype: float64"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the percentage of missing values for each column\n",
    "missing_percentage = merged_train.isnull().mean() * 100\n",
    "\n",
    "# Display columns with missing values (if any)\n",
    "missing_percentage[missing_percentage > 0].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c3be7b-0793-462b-bf4c-a7eddc62d918",
   "metadata": {},
   "source": [
    "The sell_price is a crucial feature for predicting sales revenue. Dropping a quarter of our dataset might lead to a significant loss of information, which is undesirable. Instead, we should look to impute these missing values.\n",
    "\n",
    "There are various strategies to impute missing values:\n",
    "\n",
    "- Mean/Median Imputation: Replace missing values with the mean or median of the column. This approach is suitable for features that have a normal distribution or when the missing data is completely random.\n",
    "- Mode Imputation: Replace missing values with the most frequent value. This is more applicable for categorical data.\n",
    "- Time Series Imputation: For time series data, one can use methods like forward fill or backward fill."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "054416b2-1179-414c-b4a6-aeeea1135799",
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 358. MiB for an array with shape (46985090,) and data type int64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[72], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Get unique combinations of store_id and item_id\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m unique_combinations \u001b[38;5;241m=\u001b[39m \u001b[43mmerged_train\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mstore_id\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mitem_id\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop_duplicates\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mvalues\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Loop through each combination and impute missing values\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m store, item \u001b[38;5;129;01min\u001b[39;00m unique_combinations:\n",
      "File \u001b[1;32m~\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\ml-pred-forecasting-Wb5fgStY-py3.11\\Lib\\site-packages\\pandas\\core\\frame.py:6532\u001b[0m, in \u001b[0;36mDataFrame.drop_duplicates\u001b[1;34m(self, subset, keep, inplace, ignore_index)\u001b[0m\n\u001b[0;32m   6529\u001b[0m inplace \u001b[38;5;241m=\u001b[39m validate_bool_kwarg(inplace, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minplace\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   6530\u001b[0m ignore_index \u001b[38;5;241m=\u001b[39m validate_bool_kwarg(ignore_index, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore_index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 6532\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m[\u001b[38;5;241m-\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mduplicated\u001b[49m\u001b[43m(\u001b[49m\u001b[43msubset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep\u001b[49m\u001b[43m)\u001b[49m]\n\u001b[0;32m   6533\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ignore_index:\n\u001b[0;32m   6534\u001b[0m     result\u001b[38;5;241m.\u001b[39mindex \u001b[38;5;241m=\u001b[39m default_index(\u001b[38;5;28mlen\u001b[39m(result))\n",
      "File \u001b[1;32m~\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\ml-pred-forecasting-Wb5fgStY-py3.11\\Lib\\site-packages\\pandas\\core\\frame.py:6672\u001b[0m, in \u001b[0;36mDataFrame.duplicated\u001b[1;34m(self, subset, keep)\u001b[0m\n\u001b[0;32m   6670\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   6671\u001b[0m     vals \u001b[38;5;241m=\u001b[39m (col\u001b[38;5;241m.\u001b[39mvalues \u001b[38;5;28;01mfor\u001b[39;00m name, col \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m subset)\n\u001b[1;32m-> 6672\u001b[0m     labels, shape \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvals\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   6674\u001b[0m     ids \u001b[38;5;241m=\u001b[39m get_group_index(\n\u001b[0;32m   6675\u001b[0m         labels,\n\u001b[0;32m   6676\u001b[0m         \u001b[38;5;66;03m# error: Argument 1 to \"tuple\" has incompatible type \"List[_T]\";\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   6680\u001b[0m         xnull\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   6681\u001b[0m     )\n\u001b[0;32m   6682\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_constructor_sliced(duplicated(ids, keep), index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[1;32m~\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\ml-pred-forecasting-Wb5fgStY-py3.11\\Lib\\site-packages\\pandas\\core\\frame.py:6640\u001b[0m, in \u001b[0;36mDataFrame.duplicated.<locals>.f\u001b[1;34m(vals)\u001b[0m\n\u001b[0;32m   6639\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mf\u001b[39m(vals) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[np\u001b[38;5;241m.\u001b[39mndarray, \u001b[38;5;28mint\u001b[39m]:\n\u001b[1;32m-> 6640\u001b[0m     labels, shape \u001b[38;5;241m=\u001b[39m \u001b[43malgorithms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfactorize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvals\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize_hint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   6641\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m labels\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mi8\u001b[39m\u001b[38;5;124m\"\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m), \u001b[38;5;28mlen\u001b[39m(shape)\n",
      "File \u001b[1;32m~\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\ml-pred-forecasting-Wb5fgStY-py3.11\\Lib\\site-packages\\pandas\\core\\algorithms.py:779\u001b[0m, in \u001b[0;36mfactorize\u001b[1;34m(values, sort, use_na_sentinel, size_hint)\u001b[0m\n\u001b[0;32m    776\u001b[0m             \u001b[38;5;66;03m# Don't modify (potentially user-provided) array\u001b[39;00m\n\u001b[0;32m    777\u001b[0m             values \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mwhere(null_mask, na_value, values)\n\u001b[1;32m--> 779\u001b[0m     codes, uniques \u001b[38;5;241m=\u001b[39m \u001b[43mfactorize_array\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    780\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    781\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_na_sentinel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_na_sentinel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    782\u001b[0m \u001b[43m        \u001b[49m\u001b[43msize_hint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msize_hint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    783\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    785\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sort \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    786\u001b[0m     uniques, codes \u001b[38;5;241m=\u001b[39m safe_sort(\n\u001b[0;32m    787\u001b[0m         uniques,\n\u001b[0;32m    788\u001b[0m         codes,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    791\u001b[0m         verify\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    792\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\ml-pred-forecasting-Wb5fgStY-py3.11\\Lib\\site-packages\\pandas\\core\\algorithms.py:580\u001b[0m, in \u001b[0;36mfactorize_array\u001b[1;34m(values, use_na_sentinel, size_hint, na_value, mask)\u001b[0m\n\u001b[0;32m    577\u001b[0m hash_klass, values \u001b[38;5;241m=\u001b[39m _get_hashtable_algo(values)\n\u001b[0;32m    579\u001b[0m table \u001b[38;5;241m=\u001b[39m hash_klass(size_hint \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(values))\n\u001b[1;32m--> 580\u001b[0m uniques, codes \u001b[38;5;241m=\u001b[39m \u001b[43mtable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfactorize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    581\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[43mna_sentinel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    583\u001b[0m \u001b[43m    \u001b[49m\u001b[43mna_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    584\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    585\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_na\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_na_sentinel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    586\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    588\u001b[0m \u001b[38;5;66;03m# re-cast e.g. i8->dt64/td64, uint8->bool\u001b[39;00m\n\u001b[0;32m    589\u001b[0m uniques \u001b[38;5;241m=\u001b[39m _reconstruct_data(uniques, original\u001b[38;5;241m.\u001b[39mdtype, original)\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7023\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.StringHashTable.factorize\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:6912\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.StringHashTable._unique\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 358. MiB for an array with shape (46985090,) and data type int64"
     ]
    }
   ],
   "source": [
    "# Get unique combinations of store_id and item_id\n",
    "unique_combinations = merged_train[['store_id', 'item_id']].drop_duplicates().values\n",
    "\n",
    "# Loop through each combination and impute missing values\n",
    "for store, item in unique_combinations:\n",
    "    median_price = merged_train[(merged_train['store_id'] == store) & (merged_train['item_id'] == item)]['sell_price'].median()\n",
    "    mask = (merged_train['store_id'] == store) & (merged_train['item_id'] == item)\n",
    "    merged_train.loc[mask, 'sell_price'] = merged_train.loc[mask, 'sell_price'].fillna(median_price)\n",
    "\n",
    "# Verify if there are any missing values left\n",
    "missing_after_impute = merged_train['sell_price'].isnull().sum()\n",
    "print(f\"Number of missing values after imputation: {missing_after_impute}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
