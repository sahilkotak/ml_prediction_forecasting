{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b59b7fcd-a960-4caf-a9bf-60e88035db95",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-09T22:31:04.642737Z",
     "iopub.status.busy": "2023-10-09T22:31:04.642356Z",
     "iopub.status.idle": "2023-10-09T22:31:04.649022Z",
     "shell.execute_reply": "2023-10-09T22:31:04.648286Z",
     "shell.execute_reply.started": "2023-10-09T22:31:04.642706Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../src\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65ee6f89-be4e-4413-8748-2a42983fea30",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-09T22:31:05.650976Z",
     "iopub.status.busy": "2023-10-09T22:31:05.650591Z",
     "iopub.status.idle": "2023-10-09T22:31:08.199159Z",
     "shell.execute_reply": "2023-10-09T22:31:08.198155Z",
     "shell.execute_reply.started": "2023-10-09T22:31:05.650953Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gdown in /opt/saturncloud/envs/saturn/lib/python3.9/site-packages (4.7.1)\n",
      "Requirement already satisfied: filelock in /opt/saturncloud/envs/saturn/lib/python3.9/site-packages (from gdown) (3.12.4)\n",
      "Requirement already satisfied: requests[socks] in /opt/saturncloud/envs/saturn/lib/python3.9/site-packages (from gdown) (2.31.0)\n",
      "Requirement already satisfied: six in /opt/saturncloud/envs/saturn/lib/python3.9/site-packages (from gdown) (1.16.0)\n",
      "Requirement already satisfied: tqdm in /opt/saturncloud/envs/saturn/lib/python3.9/site-packages (from gdown) (4.66.1)\n",
      "Requirement already satisfied: beautifulsoup4 in /opt/saturncloud/envs/saturn/lib/python3.9/site-packages (from gdown) (4.12.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/saturncloud/envs/saturn/lib/python3.9/site-packages (from beautifulsoup4->gdown) (2.3.2.post1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/saturncloud/envs/saturn/lib/python3.9/site-packages (from requests[socks]->gdown) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/saturncloud/envs/saturn/lib/python3.9/site-packages (from requests[socks]->gdown) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/saturncloud/envs/saturn/lib/python3.9/site-packages (from requests[socks]->gdown) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/saturncloud/envs/saturn/lib/python3.9/site-packages (from requests[socks]->gdown) (2023.7.22)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /opt/saturncloud/envs/saturn/lib/python3.9/site-packages (from requests[socks]->gdown) (1.7.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75b9ec06-f260-4ead-aa11-bd4b26492d6d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-09T22:31:08.200942Z",
     "iopub.status.busy": "2023-10-09T22:31:08.200642Z",
     "iopub.status.idle": "2023-10-09T22:31:35.055427Z",
     "shell.execute_reply": "2023-10-09T22:31:35.054687Z",
     "shell.execute_reply.started": "2023-10-09T22:31:08.200918Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=14X7usabTceBqo5d1-pcR9m5bbjoUPIzR\n",
      "To: /home/jovyan/workspace/ml_prediction_forecasting/notebooks/predictive/temp.csv\n",
      "100%|██████████| 46.2k/46.2k [00:00<00:00, 2.30MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1MsX7XJAAj-3whCKV9ZNAwO_h3hGwpx3q\n",
      "To: /home/jovyan/workspace/ml_prediction_forecasting/notebooks/predictive/temp.csv\n",
      "100%|██████████| 5.50k/5.50k [00:00<00:00, 6.62MB/s]\n",
      "Downloading...\n",
      "From (uriginal): https://drive.google.com/uc?id=1Sxtwiy9PdGq_xxxVAPAWu7M_XqCKh0Lr\n",
      "From (redirected): https://drive.google.com/uc?id=1Sxtwiy9PdGq_xxxVAPAWu7M_XqCKh0Lr&confirm=t&uuid=ae5b6d57-6c39-4248-85a5-4ba273d9384c\n",
      "To: /home/jovyan/workspace/ml_prediction_forecasting/notebooks/predictive/temp.csv\n",
      "100%|██████████| 203M/203M [00:06<00:00, 32.1MB/s] \n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1Zu_bRhPg0CnHW-ihAHurzWTk7uJ11uRC\n",
      "To: /home/jovyan/workspace/ml_prediction_forecasting/notebooks/predictive/temp.csv\n",
      "100%|██████████| 24.7M/24.7M [00:00<00:00, 76.8MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1kSciVM9LLaLsJXiY1vaAO-UtBzZ-YE6l\n",
      "To: /home/jovyan/workspace/ml_prediction_forecasting/notebooks/predictive/temp.csv\n",
      "100%|██████████| 97.1M/97.1M [00:01<00:00, 50.5MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         date  wm_yr_wk    d\n",
      "0  2011-01-29     11101  d_1\n",
      "1  2011-01-30     11101  d_2\n",
      "2  2011-01-31     11101  d_3\n",
      "3  2011-02-01     11101  d_4\n",
      "4  2011-02-02     11101  d_5\n",
      "         date     event_name event_type\n",
      "0  2011-02-06      SuperBowl   Sporting\n",
      "1  2011-02-14  ValentinesDay   Cultural\n",
      "2  2011-02-21  PresidentsDay   National\n",
      "3  2011-03-09      LentStart  Religious\n",
      "4  2011-03-16      LentWeek2  Religious\n",
      "  store_id        item_id  wm_yr_wk  sell_price\n",
      "0     CA_1  HOBBIES_1_001     11325        9.58\n",
      "1     CA_1  HOBBIES_1_001     11326        9.58\n",
      "2     CA_1  HOBBIES_1_001     11327        8.26\n",
      "3     CA_1  HOBBIES_1_001     11328        8.26\n",
      "4     CA_1  HOBBIES_1_001     11329        8.26\n",
      "   d_1542  d_1543  d_1544  d_1545  d_1546  d_1547  d_1548  d_1549  d_1550  \\\n",
      "0       0       1       0       2       1       0       2       0       1   \n",
      "1       0       0       0       0       0       0       0       0       1   \n",
      "2       0       0       0       0       0       1       0       0       0   \n",
      "3       4       1       0       1       3       5       2       3       0   \n",
      "4       3       0       0       1       1       0       2       0       2   \n",
      "\n",
      "   d_1551  ...  d_1932  d_1933  d_1934  d_1935  d_1936  d_1937  d_1938  \\\n",
      "0       0  ...       2       4       0       0       0       0       3   \n",
      "1       0  ...       0       1       2       1       1       0       0   \n",
      "2       0  ...       1       0       2       0       0       0       2   \n",
      "3       2  ...       1       1       0       4       0       1       3   \n",
      "4       1  ...       0       0       0       2       1       0       0   \n",
      "\n",
      "   d_1939  d_1940  d_1941  \n",
      "0       3       0       1  \n",
      "1       0       0       0  \n",
      "2       3       0       1  \n",
      "3       0       2       6  \n",
      "4       2       1       0  \n",
      "\n",
      "[5 rows x 400 columns]\n",
      "                              id        item_id    dept_id   cat_id store_id  \\\n",
      "0  HOBBIES_1_001_CA_1_evaluation  HOBBIES_1_001  HOBBIES_1  HOBBIES     CA_1   \n",
      "1  HOBBIES_1_002_CA_1_evaluation  HOBBIES_1_002  HOBBIES_1  HOBBIES     CA_1   \n",
      "2  HOBBIES_1_003_CA_1_evaluation  HOBBIES_1_003  HOBBIES_1  HOBBIES     CA_1   \n",
      "3  HOBBIES_1_004_CA_1_evaluation  HOBBIES_1_004  HOBBIES_1  HOBBIES     CA_1   \n",
      "4  HOBBIES_1_005_CA_1_evaluation  HOBBIES_1_005  HOBBIES_1  HOBBIES     CA_1   \n",
      "\n",
      "  state_id  d_1  d_2  d_3  d_4  ...  d_1532  d_1533  d_1534  d_1535  d_1536  \\\n",
      "0       CA    0    0    0    0  ...       1       1       1       0       1   \n",
      "1       CA    0    0    0    0  ...       0       0       0       0       0   \n",
      "2       CA    0    0    0    0  ...       0       0       1       0       0   \n",
      "3       CA    0    0    0    0  ...       8       2       0       8       2   \n",
      "4       CA    0    0    0    0  ...       2       0       1       3       2   \n",
      "\n",
      "   d_1537  d_1538  d_1539  d_1540  d_1541  \n",
      "0       0       1       0       0       1  \n",
      "1       0       0       0       1       0  \n",
      "2       0       0       0       0       0  \n",
      "3       3       1       1       3       8  \n",
      "4       1       1       2       2       3  \n",
      "\n",
      "[5 rows x 1547 columns]\n"
     ]
    }
   ],
   "source": [
    "import gdown\n",
    "import pandas as pd\n",
    "\n",
    "# URLs for the datasets\n",
    "urls = [\n",
    "    \"https://drive.google.com/uc?id=14X7usabTceBqo5d1-pcR9m5bbjoUPIzR\",\n",
    "    \"https://drive.google.com/uc?id=1MsX7XJAAj-3whCKV9ZNAwO_h3hGwpx3q\",\n",
    "    \"https://drive.google.com/uc?id=1Sxtwiy9PdGq_xxxVAPAWu7M_XqCKh0Lr\",\n",
    "    \"https://drive.google.com/uc?id=1Zu_bRhPg0CnHW-ihAHurzWTk7uJ11uRC\",\n",
    "    \"https://drive.google.com/uc?id=1kSciVM9LLaLsJXiY1vaAO-UtBzZ-YE6l\"\n",
    "]\n",
    "\n",
    "dfs = []  # list to hold the dataframes\n",
    "\n",
    "# Download and read each dataset into a dataframe\n",
    "for url in urls:\n",
    "    gdown.download(url, 'temp.csv', quiet=False)\n",
    "    df = pd.read_csv('temp.csv')\n",
    "    dfs.append(df)\n",
    "\n",
    "# Assign each dataframe to a variable\n",
    "df1, df2, df3, df4, df5 = dfs\n",
    "\n",
    "# View the first few rows of each dataframe to understand their structure\n",
    "print(df1.head())\n",
    "print(df2.head())\n",
    "print(df3.head())\n",
    "print(df4.head())\n",
    "print(df5.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd6760c9-02c6-4833-a401-ef82a0c98fe1",
   "metadata": {},
   "source": [
    "From the output above, it appears that we have the following data structures:\n",
    "\n",
    "- Calendar Data (df1): Contains the date, the corresponding week identifier, and a day identifier.\n",
    "- Events Data (df2): Contains the date, event name, and event type.\n",
    "- Item Price Data (df3): Contains the store ID, item ID, week identifier, and sell price.\n",
    "- Evaluation Data (df4): Appears to contain sales data for multiple days, but the structure isn't clear just from the first few rows. Each column d_xxx seems to represent the sales for a specific day.\n",
    "- Training Data (df5): Contains an ID, item ID, department ID, category ID, store ID, state ID, and sales data for multiple days.\n",
    "\n",
    "Observations & Proposed Changes:\n",
    "\n",
    "- The Calendar Data looks fine.\n",
    "- The Events Data seems appropriate as well.\n",
    "- Item Price Data is in good shape, containing the required details.\n",
    "\n",
    "- Evaluation Data (df4):\n",
    "  - This seems to be in wide format with sales data for each day represented as a separate column. It might be helpful to melt this dataframe to a long format for easier analysis and modeling.\n",
    "\n",
    "\n",
    "- Training Data (df5):\n",
    "  - This dataframe is also in a wide format. It might be beneficial to melt this into a long format similar to the evaluation data.\n",
    "\n",
    "To transform the wide dataframe into a long format, we'll use the melt function from pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e738764c-3037-4338-a7f6-d7a9debaccea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-09T22:31:35.057759Z",
     "iopub.status.busy": "2023-10-09T22:31:35.057152Z",
     "iopub.status.idle": "2023-10-09T22:31:40.612776Z",
     "shell.execute_reply": "2023-10-09T22:31:40.612154Z",
     "shell.execute_reply.started": "2023-10-09T22:31:35.057725Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              id        item_id    dept_id   cat_id store_id  \\\n",
      "0  HOBBIES_1_001_CA_1_evaluation  HOBBIES_1_001  HOBBIES_1  HOBBIES     CA_1   \n",
      "1  HOBBIES_1_002_CA_1_evaluation  HOBBIES_1_002  HOBBIES_1  HOBBIES     CA_1   \n",
      "2  HOBBIES_1_003_CA_1_evaluation  HOBBIES_1_003  HOBBIES_1  HOBBIES     CA_1   \n",
      "3  HOBBIES_1_004_CA_1_evaluation  HOBBIES_1_004  HOBBIES_1  HOBBIES     CA_1   \n",
      "4  HOBBIES_1_005_CA_1_evaluation  HOBBIES_1_005  HOBBIES_1  HOBBIES     CA_1   \n",
      "\n",
      "  state_id    d  sales  \n",
      "0       CA  d_1      0  \n",
      "1       CA  d_1      0  \n",
      "2       CA  d_1      0  \n",
      "3       CA  d_1      0  \n",
      "4       CA  d_1      0  \n"
     ]
    }
   ],
   "source": [
    "# Melt the training data\n",
    "df5_melted = pd.melt(df5, \n",
    "                     id_vars=['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'], \n",
    "                     var_name='d', \n",
    "                     value_name='sales')\n",
    "\n",
    "# Display the first few rows of the transformed dataframe\n",
    "print(df5_melted.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46de1cc2-d78e-43e5-a069-a8365413c5ae",
   "metadata": {},
   "source": [
    "To verify the transformation and ensure correctness, we will perform the following checks:\n",
    "\n",
    "- Dimensionality Check: Ensure that the total number of rows in the melted dataframes matches the expected count based on the original dataframes.\n",
    "    - For df5, the number of rows in the melted version should be: \"number of unique items × number of days in the dataset\"\n",
    "\n",
    "- Value Check: Sample a few item-day combinations and compare the sales values in the melted dataframe with the corresponding values in the wide dataframe. This ensures that the melt operation hasn't inadvertently changed any values.\n",
    "\n",
    "- Missing Values: Ensure that there are no missing values in the sales column of the melted dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a11c8e8c-7a8f-4a2a-9333-68892f27edc1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-09T22:31:40.614168Z",
     "iopub.status.busy": "2023-10-09T22:31:40.613641Z",
     "iopub.status.idle": "2023-10-09T22:31:50.497954Z",
     "shell.execute_reply": "2023-10-09T22:31:50.497334Z",
     "shell.execute_reply.started": "2023-10-09T22:31:40.614145Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All checks passed!\n"
     ]
    }
   ],
   "source": [
    "# Dimensionality Check\n",
    "assert len(df5_melted) == len(df5) * len(df5.columns[6:]), \"Mismatch in the number of rows for df5_melted\"\n",
    "\n",
    "# Value Check for df5\n",
    "# Sample a random item-day combination\n",
    "sample_item = df5_melted['item_id'].sample().iloc[0]\n",
    "sample_day = df5_melted['d'].sample().iloc[0]\n",
    "assert df5_melted[(df5_melted['item_id'] == sample_item) & (df5_melted['d'] == sample_day)]['sales'].iloc[0] == df5[df5['item_id'] == sample_item][sample_day].iloc[0], \"Mismatch in sales value for df5_melted\"\n",
    "\n",
    "# Missing Values Check\n",
    "assert df5_melted['sales'].isna().sum() == 0, \"Missing values detected in df5_melted['sales']\"\n",
    "\n",
    "print(\"All checks passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4372bb56-f5a6-4023-b7df-6e47b1adcce9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-09T22:31:50.499758Z",
     "iopub.status.busy": "2023-10-09T22:31:50.499465Z",
     "iopub.status.idle": "2023-10-09T22:31:51.661151Z",
     "shell.execute_reply": "2023-10-09T22:31:51.660489Z",
     "shell.execute_reply.started": "2023-10-09T22:31:50.499738Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data exported!\n"
     ]
    }
   ],
   "source": [
    "# Exporting Weekly Sell Price data to .parquet for future use\n",
    "\n",
    "df3.to_parquet(\"weekly_sell_price.parquet\", index=False)\n",
    "print(f\"Data exported!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26709070-1eac-4cdb-b1c0-bb9fbb140547",
   "metadata": {},
   "source": [
    "### Joining Dataframes:\n",
    "- First, we'll join the training data (df5_melted) with the calendar data (df1) on the 'd' column. This will give us date-related information for each sales entry. Then, we'll join this resulting dataframe with the item price data (df3) on 'item_id', 'store_id', and 'wm_yr_wk' to incorporate the selling price for each item on each day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee88df55-6dff-4301-827b-24d926a3116d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-09T22:31:51.662253Z",
     "iopub.status.busy": "2023-10-09T22:31:51.661996Z",
     "iopub.status.idle": "2023-10-09T22:32:00.756984Z",
     "shell.execute_reply": "2023-10-09T22:32:00.756073Z",
     "shell.execute_reply.started": "2023-10-09T22:31:51.662233Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              id        item_id    dept_id   cat_id store_id  \\\n",
      "0  HOBBIES_1_001_CA_1_evaluation  HOBBIES_1_001  HOBBIES_1  HOBBIES     CA_1   \n",
      "1  HOBBIES_1_002_CA_1_evaluation  HOBBIES_1_002  HOBBIES_1  HOBBIES     CA_1   \n",
      "2  HOBBIES_1_003_CA_1_evaluation  HOBBIES_1_003  HOBBIES_1  HOBBIES     CA_1   \n",
      "3  HOBBIES_1_004_CA_1_evaluation  HOBBIES_1_004  HOBBIES_1  HOBBIES     CA_1   \n",
      "4  HOBBIES_1_005_CA_1_evaluation  HOBBIES_1_005  HOBBIES_1  HOBBIES     CA_1   \n",
      "\n",
      "  state_id    d  sales        date  wm_yr_wk  \n",
      "0       CA  d_1      0  2011-01-29     11101  \n",
      "1       CA  d_1      0  2011-01-29     11101  \n",
      "2       CA  d_1      0  2011-01-29     11101  \n",
      "3       CA  d_1      0  2011-01-29     11101  \n",
      "4       CA  d_1      0  2011-01-29     11101  \n"
     ]
    }
   ],
   "source": [
    "# Filter df1 to only include rows up to d_1541\n",
    "df1_filtered = df1[df1['d'].isin(df5_melted['d'])]\n",
    "\n",
    "# Merge the dataframes\n",
    "merged_data = pd.merge(df5_melted, df1_filtered, on='d', how='left')\n",
    "\n",
    "# Display the first few rows of the merged dataframe\n",
    "print(merged_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "333d5e59-d87b-40c0-bcab-6fec86041840",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-09T22:32:00.758221Z",
     "iopub.status.busy": "2023-10-09T22:32:00.757938Z",
     "iopub.status.idle": "2023-10-09T22:32:26.925495Z",
     "shell.execute_reply": "2023-10-09T22:32:26.924857Z",
     "shell.execute_reply.started": "2023-10-09T22:32:00.758202Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              id        item_id    dept_id   cat_id store_id  \\\n",
      "0  HOBBIES_1_001_CA_1_evaluation  HOBBIES_1_001  HOBBIES_1  HOBBIES     CA_1   \n",
      "1  HOBBIES_1_002_CA_1_evaluation  HOBBIES_1_002  HOBBIES_1  HOBBIES     CA_1   \n",
      "2  HOBBIES_1_003_CA_1_evaluation  HOBBIES_1_003  HOBBIES_1  HOBBIES     CA_1   \n",
      "3  HOBBIES_1_004_CA_1_evaluation  HOBBIES_1_004  HOBBIES_1  HOBBIES     CA_1   \n",
      "4  HOBBIES_1_005_CA_1_evaluation  HOBBIES_1_005  HOBBIES_1  HOBBIES     CA_1   \n",
      "\n",
      "  state_id    d  sales        date  wm_yr_wk  sell_price  \n",
      "0       CA  d_1      0  2011-01-29     11101         NaN  \n",
      "1       CA  d_1      0  2011-01-29     11101         NaN  \n",
      "2       CA  d_1      0  2011-01-29     11101         NaN  \n",
      "3       CA  d_1      0  2011-01-29     11101         NaN  \n",
      "4       CA  d_1      0  2011-01-29     11101         NaN  \n"
     ]
    }
   ],
   "source": [
    "merged_train = pd.merge(merged_data, df3, on=['store_id', 'item_id', 'wm_yr_wk'], how='left')\n",
    "print(merged_train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "331d180f-6d49-4c0e-9c85-f948b554c3b0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-09T22:32:26.926640Z",
     "iopub.status.busy": "2023-10-09T22:32:26.926359Z",
     "iopub.status.idle": "2023-10-09T22:32:36.889609Z",
     "shell.execute_reply": "2023-10-09T22:32:36.888960Z",
     "shell.execute_reply.started": "2023-10-09T22:32:26.926620Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Merge train set with Events dataframe\n",
    "merged_train = pd.merge(merged_train, df2, on='date', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ec840a-41f4-47cc-982a-960f84937925",
   "metadata": {},
   "source": [
    "# Data Investigation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a029bf5-8198-4600-ab2b-c6ba882940b5",
   "metadata": {},
   "source": [
    "### Check for any data quality issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee61ed14-0650-4544-99e5-9ed95037ac92",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-09T11:36:58.176306Z",
     "iopub.status.busy": "2023-10-09T11:36:58.176022Z",
     "iopub.status.idle": "2023-10-09T11:37:11.469055Z",
     "shell.execute_reply": "2023-10-09T11:37:11.468439Z",
     "shell.execute_reply.started": "2023-10-09T11:36:58.176284Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [store_id, item_id, wm_yr_wk, sell_price]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# Sample missing data\n",
    "sample_missing = merged_train[merged_train['sell_price'].isna()].sample()\n",
    "sample_item = sample_missing['item_id'].iloc[0]\n",
    "sample_store = sample_missing['store_id'].iloc[0]\n",
    "sample_week = sample_missing['wm_yr_wk'].iloc[0]\n",
    "\n",
    "# Check in original price data\n",
    "price_check = df3[(df3['item_id'] == sample_item) & (df3['store_id'] == sample_store) & (df3['wm_yr_wk'] == sample_week)]\n",
    "print(price_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f00c20a3-ed3d-4c59-bc58-609b8ef97db1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-09T11:37:11.470217Z",
     "iopub.status.busy": "2023-10-09T11:37:11.469924Z",
     "iopub.status.idle": "2023-10-09T11:37:14.162483Z",
     "shell.execute_reply": "2023-10-09T11:37:14.161886Z",
     "shell.execute_reply.started": "2023-10-09T11:37:11.470198Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weeks with missing prices for HOUSEHOLD_1_159: \n",
      "[11101, 11102, 11103, 11104, 11105, 11106, 11107, 11108, 11109, 11110, 11111, 11112, 11113, 11114, 11115, 11116, 11117, 11118, 11119, 11120, 11121, 11122, 11123, 11124, 11125, 11126, 11127, 11128, 11129, 11130, 11131, 11132, 11133, 11134, 11135, 11136, 11137, 11138, 11139, 11140, 11141, 11142, 11143, 11144, 11145, 11146, 11147, 11148, 11149, 11150, 11151, 11152, 11201, 11202, 11203, 11204, 11205, 11206, 11207, 11208, 11209, 11210, 11211, 11212, 11213, 11214, 11215, 11216, 11217, 11218, 11219, 11220, 11221, 11222, 11223, 11224, 11225, 11226, 11227, 11228, 11229, 11230, 11231, 11232, 11233, 11234, 11235, 11236, 11237, 11238, 11239, 11240, 11241, 11242, 11243, 11244, 11245, 11246, 11247, 11248, 11249, 11250, 11251, 11252, 11301, 11302, 11303, 11304, 11305, 11306, 11307, 11308, 11309, 11310, 11311, 11312, 11313, 11314, 11315, 11316, 11317, 11318, 11319, 11320, 11321, 11322, 11323, 11324, 11325, 11326, 11327, 11328, 11329, 11330, 11331, 11332, 11333, 11334, 11335, 11336, 11337, 11338, 11339, 11340, 11341, 11342, 11343, 11344, 11345, 11346, 11347, 11348, 11349, 11350, 11351, 11352, 11353, 11401, 11402, 11403, 11404, 11405, 11406, 11407, 11408, 11409, 11410, 11411, 11412, 11413, 11414, 11415, 11416, 11417, 11418, 11419, 11420, 11421, 11422, 11423, 11424, 11425, 11426, 11427, 11428, 11429, 11430, 11431, 11432, 11433, 11434, 11435, 11436, 11437, 11438, 11439, 11440, 11441, 11442, 11443, 11444, 11445, 11446, 11447, 11448, 11449, 11450, 11451, 11452, 11501, 11502, 11503, 11504, 11505, 11506, 11507, 11508, 11509, 11510, 11511, 11512]\n"
     ]
    }
   ],
   "source": [
    "missing_weeks_item = merged_train[(merged_train['item_id'] == 'HOUSEHOLD_1_159') & (merged_train['sell_price'].isna())]['wm_yr_wk'].unique()\n",
    "print(f\"Weeks with missing prices for HOUSEHOLD_1_159: \\n{sorted(missing_weeks_item)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8765c197-41cf-4784-8fbe-39484d80bbd8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-09T11:37:14.163618Z",
     "iopub.status.busy": "2023-10-09T11:37:14.163332Z",
     "iopub.status.idle": "2023-10-09T11:38:02.508650Z",
     "shell.execute_reply": "2023-10-09T11:38:02.508045Z",
     "shell.execute_reply.started": "2023-10-09T11:37:14.163595Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of combinations in merged data not present in price data: 1752423\n"
     ]
    }
   ],
   "source": [
    "# Unique combinations in the merged dataframe\n",
    "merged_combinations = set(merged_train[['item_id', 'store_id', 'wm_yr_wk']].itertuples(index=False))\n",
    "\n",
    "# Unique combinations in the price dataframe\n",
    "price_combinations = set(df3[['item_id', 'store_id', 'wm_yr_wk']].itertuples(index=False))\n",
    "\n",
    "# Find combinations in the merged dataframe that are not in the price dataframe\n",
    "missing_combinations = merged_combinations - price_combinations\n",
    "\n",
    "print(f\"Number of combinations in merged data not present in price data: {len(missing_combinations)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bdbae9bf-ce1d-475c-917e-0c116deeca19",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-09T11:38:02.511122Z",
     "iopub.status.busy": "2023-10-09T11:38:02.510852Z",
     "iopub.status.idle": "2023-10-09T11:38:02.637125Z",
     "shell.execute_reply": "2023-10-09T11:38:02.636510Z",
     "shell.execute_reply.started": "2023-10-09T11:38:02.511100Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample missing combinations:\n",
      " [Pandas(item_id='FOODS_3_531', store_id='WI_2', wm_yr_wk=11109), Pandas(item_id='FOODS_1_082', store_id='TX_1', wm_yr_wk=11222), Pandas(item_id='FOODS_1_169', store_id='TX_3', wm_yr_wk=11208), Pandas(item_id='FOODS_2_111', store_id='CA_4', wm_yr_wk=11138), Pandas(item_id='HOUSEHOLD_1_405', store_id='CA_1', wm_yr_wk=11326)]\n"
     ]
    }
   ],
   "source": [
    "sample_missing_combinations = list(missing_combinations)[:5]\n",
    "print(\"Sample missing combinations:\\n\", sample_missing_combinations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c07f47f2-8627-4984-b274-18ea934952c8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-09T11:38:02.638209Z",
     "iopub.status.busy": "2023-10-09T11:38:02.637954Z",
     "iopub.status.idle": "2023-10-09T11:38:03.902841Z",
     "shell.execute_reply": "2023-10-09T11:38:03.902233Z",
     "shell.execute_reply.started": "2023-10-09T11:38:02.638189Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data types in merged_train:\n",
      " item_id     object\n",
      "store_id    object\n",
      "wm_yr_wk     int64\n",
      "dtype: object\n",
      "\n",
      "Data types in df3:\n",
      " item_id     object\n",
      "store_id    object\n",
      "wm_yr_wk     int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(\"Data types in merged_train:\\n\", merged_train[['item_id', 'store_id', 'wm_yr_wk']].dtypes)\n",
    "print(\"\\nData types in df3:\\n\", df3[['item_id', 'store_id', 'wm_yr_wk']].dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "af0cddf5-0a6c-4360-bf5e-0a18a356892c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-09T11:38:03.903995Z",
     "iopub.status.busy": "2023-10-09T11:38:03.903686Z",
     "iopub.status.idle": "2023-10-09T11:38:05.226363Z",
     "shell.execute_reply": "2023-10-09T11:38:05.225743Z",
     "shell.execute_reply.started": "2023-10-09T11:38:03.903972Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [store_id, item_id, wm_yr_wk, sell_price]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "duplicates = df3[df3.duplicated(subset=['store_id', 'item_id', 'wm_yr_wk'], keep=False)]\n",
    "print(duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7a4b34ef-795b-41f2-81cc-f289ea71333a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-09T12:20:42.410479Z",
     "iopub.status.busy": "2023-10-09T12:20:42.410106Z",
     "iopub.status.idle": "2023-10-09T12:20:42.417080Z",
     "shell.execute_reply": "2023-10-09T12:20:42.416444Z",
     "shell.execute_reply.started": "2023-10-09T12:20:42.410454Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     store_id        item_id  wm_yr_wk  sell_price\n",
      "0        CA_1  HOBBIES_1_001     11325        9.58\n",
      "1        CA_1  HOBBIES_1_001     11326        9.58\n",
      "2        CA_1  HOBBIES_1_001     11327        8.26\n",
      "3        CA_1  HOBBIES_1_001     11328        8.26\n",
      "4        CA_1  HOBBIES_1_001     11329        8.26\n",
      "...       ...            ...       ...         ...\n",
      "2495     CA_1  HOBBIES_1_011     11445        3.48\n",
      "2496     CA_1  HOBBIES_1_011     11446        3.48\n",
      "2497     CA_1  HOBBIES_1_011     11447        3.48\n",
      "2498     CA_1  HOBBIES_1_011     11448        3.48\n",
      "2499     CA_1  HOBBIES_1_011     11449        3.48\n",
      "\n",
      "[2500 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df3.head(2500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dc599890-caa9-4574-9c4a-00af5fb0fae2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-09T11:38:05.234671Z",
     "iopub.status.busy": "2023-10-09T11:38:05.234427Z",
     "iopub.status.idle": "2023-10-09T11:38:47.893571Z",
     "shell.execute_reply": "2023-10-09T11:38:47.892957Z",
     "shell.execute_reply.started": "2023-10-09T11:38:05.234653Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [store_id, item_id, wm_yr_wk, sell_price]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "sample_check = df3[df3[['store_id', 'item_id', 'wm_yr_wk']].apply(tuple, axis=1).isin(sample_missing_combinations)]\n",
    "print(sample_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7d8d9ede-9317-4d7a-b25d-94e0d3a42b7b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-08T05:16:25.891151Z",
     "iopub.status.busy": "2023-10-08T05:16:25.890838Z",
     "iopub.status.idle": "2023-10-08T05:16:26.494193Z",
     "shell.execute_reply": "2023-10-08T05:16:26.493573Z",
     "shell.execute_reply.started": "2023-10-08T05:16:25.891129Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weeks in merged_data not in df3: set()\n",
      "Weeks in df3 not in merged_data: {11520, 11521, 11522, 11523, 11524, 11525, 11526, 11527, 11528, 11529, 11530, 11531, 11532, 11533, 11534, 11535, 11536, 11537, 11538, 11539, 11540, 11541, 11542, 11543, 11544, 11545, 11546, 11547, 11548, 11549, 11550, 11551, 11552, 11601, 11602, 11603, 11604, 11605, 11606, 11607, 11608, 11609, 11610, 11611, 11612, 11613, 11614, 11615, 11616, 11617, 11618, 11619, 11620, 11621, 11513, 11514, 11515, 11516, 11517, 11518, 11519}\n"
     ]
    }
   ],
   "source": [
    "missing_weeks_in_df3 = set(merged_data['wm_yr_wk'].unique()) - set(df3['wm_yr_wk'].unique())\n",
    "print(\"Weeks in merged_data not in df3:\", missing_weeks_in_df3)\n",
    "\n",
    "missing_weeks_in_merged_data = set(df3['wm_yr_wk'].unique()) - set(merged_data['wm_yr_wk'].unique())\n",
    "print(\"Weeks in df3 not in merged_data:\", missing_weeks_in_merged_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "25ed8d73-2239-4a63-be14-fceb4b41bb2e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-08T05:16:26.495400Z",
     "iopub.status.busy": "2023-10-08T05:16:26.495088Z",
     "iopub.status.idle": "2023-10-08T05:17:22.093797Z",
     "shell.execute_reply": "2023-10-08T05:17:22.093168Z",
     "shell.execute_reply.started": "2023-10-08T05:16:26.495377Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of combinations in merged_data not present in df3: 1752423\n",
      "Sample missing combinations: [Pandas(store_id='TX_3', item_id='FOODS_2_338', wm_yr_wk=11212), Pandas(store_id='CA_2', item_id='HOUSEHOLD_1_378', wm_yr_wk=11329), Pandas(store_id='TX_3', item_id='FOODS_2_023', wm_yr_wk=11353), Pandas(store_id='CA_1', item_id='FOODS_1_068', wm_yr_wk=11128), Pandas(store_id='CA_2', item_id='HOUSEHOLD_2_344', wm_yr_wk=11209)]\n"
     ]
    }
   ],
   "source": [
    "# Get unique combinations in both dataframes\n",
    "merged_data_combinations = set(merged_data[['store_id', 'item_id', 'wm_yr_wk']].itertuples(index=False))\n",
    "df3_combinations = set(df3[['store_id', 'item_id', 'wm_yr_wk']].itertuples(index=False))\n",
    "\n",
    "# Find combinations in merged_data that aren't in df3\n",
    "missing_combinations = merged_data_combinations - df3_combinations\n",
    "\n",
    "print(\"Number of combinations in merged_data not present in df3:\", len(missing_combinations))\n",
    "print(\"Sample missing combinations:\", list(missing_combinations)[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3db56c-2dfa-43bd-bf4b-0e2329ee302e",
   "metadata": {},
   "source": [
    "#### Check the Percentage of Missing Values:\n",
    "- Let's see how widespread the missing values are in the sell_price column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bba49707-d6e3-404b-a943-88572eda956d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-08T05:17:22.094948Z",
     "iopub.status.busy": "2023-10-08T05:17:22.094633Z",
     "iopub.status.idle": "2023-10-08T05:17:22.175400Z",
     "shell.execute_reply": "2023-10-08T05:17:22.174778Z",
     "shell.execute_reply.started": "2023-10-08T05:17:22.094926Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of missing values in sell_price: 26.09%\n"
     ]
    }
   ],
   "source": [
    "missing_percentage = (merged_train['sell_price'].isna().sum() / len(merged_train)) * 100\n",
    "print(f\"Percentage of missing values in sell_price: {missing_percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1cf5c739-a3ea-41b2-9b24-88beafe97b77",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-08T05:17:22.176539Z",
     "iopub.status.busy": "2023-10-08T05:17:22.176233Z",
     "iopub.status.idle": "2023-10-08T05:17:23.843002Z",
     "shell.execute_reply": "2023-10-08T05:17:23.842341Z",
     "shell.execute_reply.started": "2023-10-08T05:17:22.176518Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "store_id      0\n",
      "item_id       0\n",
      "wm_yr_wk      0\n",
      "sell_price    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "missing_values_df3 = df3.isnull().sum()\n",
    "print(missing_values_df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f3e57462-e13b-41fe-918d-e72d14166925",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-08T05:17:23.844183Z",
     "iopub.status.busy": "2023-10-08T05:17:23.843949Z",
     "iopub.status.idle": "2023-10-08T05:17:23.854394Z",
     "shell.execute_reply": "2023-10-08T05:17:23.853769Z",
     "shell.execute_reply.started": "2023-10-08T05:17:23.844163Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Series([], Name: item_id, dtype: int64)\n"
     ]
    }
   ],
   "source": [
    "items_missing_prices = df3[df3['sell_price'].isnull()]['item_id'].value_counts()\n",
    "print(items_missing_prices.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "973fe897-603e-45d8-8106-c70baf402da4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-08T05:17:23.855667Z",
     "iopub.status.busy": "2023-10-08T05:17:23.855318Z",
     "iopub.status.idle": "2023-10-08T05:17:23.865850Z",
     "shell.execute_reply": "2023-10-08T05:17:23.865273Z",
     "shell.execute_reply.started": "2023-10-08T05:17:23.855643Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Series([], Name: store_id, dtype: int64)\n"
     ]
    }
   ],
   "source": [
    "stores_missing_prices = df3[df3['sell_price'].isnull()]['store_id'].value_counts()\n",
    "print(stores_missing_prices.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ec04fb78-c108-4dc8-91ba-1414651a5291",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-08T05:17:23.866948Z",
     "iopub.status.busy": "2023-10-08T05:17:23.866658Z",
     "iopub.status.idle": "2023-10-08T05:17:23.876915Z",
     "shell.execute_reply": "2023-10-08T05:17:23.876306Z",
     "shell.execute_reply.started": "2023-10-08T05:17:23.866926Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Series([], Name: wm_yr_wk, dtype: int64)\n"
     ]
    }
   ],
   "source": [
    "weeks_missing_prices = df3[df3['sell_price'].isnull()]['wm_yr_wk'].value_counts()\n",
    "print(weeks_missing_prices.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8d4569d5-190e-4f16-9d46-05907b9dd181",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-08T05:17:23.878204Z",
     "iopub.status.busy": "2023-10-08T05:17:23.877899Z",
     "iopub.status.idle": "2023-10-08T05:17:25.738502Z",
     "shell.execute_reply": "2023-10-08T05:17:25.737857Z",
     "shell.execute_reply.started": "2023-10-08T05:17:23.878181Z"
    }
   },
   "outputs": [],
   "source": [
    "unique_d_sales = merged_train['d'].nunique()\n",
    "unique_d_calendar = df1['d'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "122d429b-2ef7-48e7-9f60-bc3c7a35258a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-08T05:17:25.739756Z",
     "iopub.status.busy": "2023-10-08T05:17:25.739383Z",
     "iopub.status.idle": "2023-10-08T05:17:27.579104Z",
     "shell.execute_reply": "2023-10-08T05:17:27.578472Z",
     "shell.execute_reply.started": "2023-10-08T05:17:25.739734Z"
    }
   },
   "outputs": [],
   "source": [
    "merged_rows = merged_train.shape[0]\n",
    "unique_d_merged = merged_train['d'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bc7a62b2-a7a1-47e2-9094-c64aa64c8221",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-08T05:17:27.580553Z",
     "iopub.status.busy": "2023-10-08T05:17:27.580133Z",
     "iopub.status.idle": "2023-10-08T05:17:29.711740Z",
     "shell.execute_reply": "2023-10-08T05:17:29.710834Z",
     "shell.execute_reply.started": "2023-10-08T05:17:27.580518Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weeks with the most missing price data:\n",
      "11101    136906\n",
      "11102    129605\n",
      "11113    127096\n",
      "11103    125797\n",
      "11104    123837\n",
      "11105    122199\n",
      "11106    120092\n",
      "11107    118223\n",
      "11108    116802\n",
      "11109    115759\n",
      "Name: wm_yr_wk, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Extract weeks with missing prices from the merged data\n",
    "missing_weeks = merged_train[merged_train['sell_price'].isnull()]['wm_yr_wk'].value_counts()\n",
    "\n",
    "# Display weeks with the most missing data\n",
    "print(\"Weeks with the most missing price data:\")\n",
    "print(missing_weeks.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "22caedbc-4a30-41ca-a997-935c9b7ccb89",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-08T05:17:29.713275Z",
     "iopub.status.busy": "2023-10-08T05:17:29.712841Z",
     "iopub.status.idle": "2023-10-08T05:17:32.454765Z",
     "shell.execute_reply": "2023-10-08T05:17:32.453982Z",
     "shell.execute_reply.started": "2023-10-08T05:17:29.713240Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Items with the most missing price data:\n",
      "HOUSEHOLD_1_159    15337\n",
      "HOUSEHOLD_1_242    15228\n",
      "HOUSEHOLD_1_308    15190\n",
      "HOUSEHOLD_2_186    15049\n",
      "FOODS_3_353        15021\n",
      "HOUSEHOLD_1_489    14929\n",
      "HOUSEHOLD_1_484    14922\n",
      "HOUSEHOLD_1_534    14922\n",
      "FOODS_3_255        14894\n",
      "HOUSEHOLD_1_526    14852\n",
      "Name: item_id, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Extract items with missing prices from the merged data\n",
    "missing_items = merged_train[merged_train['sell_price'].isnull()]['item_id'].value_counts()\n",
    "\n",
    "# Display items with the most missing data\n",
    "print(\"Items with the most missing price data:\")\n",
    "print(missing_items.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "842059ab-66d0-4c4c-ab7f-8dfe0cb8703b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-08T05:17:32.456316Z",
     "iopub.status.busy": "2023-10-08T05:17:32.455904Z",
     "iopub.status.idle": "2023-10-08T05:17:35.169935Z",
     "shell.execute_reply": "2023-10-08T05:17:35.169126Z",
     "shell.execute_reply.started": "2023-10-08T05:17:32.456282Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stores with the most missing price data:\n",
      "CA_2    1544070\n",
      "WI_1    1358539\n",
      "WI_2    1272203\n",
      "CA_4    1264051\n",
      "TX_3    1180844\n",
      "CA_3    1161252\n",
      "WI_3    1147725\n",
      "CA_1    1130422\n",
      "TX_1    1121384\n",
      "TX_2    1111386\n",
      "Name: store_id, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Extract stores with missing prices from the merged data\n",
    "missing_stores = merged_train[merged_train['sell_price'].isnull()]['store_id'].value_counts()\n",
    "\n",
    "# Display stores with the most missing data\n",
    "print(\"Stores with the most missing price data:\")\n",
    "print(missing_stores.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b04f36b7-d9f0-4c0c-8bc5-af2ddd795c97",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-08T05:17:35.171513Z",
     "iopub.status.busy": "2023-10-08T05:17:35.171083Z",
     "iopub.status.idle": "2023-10-08T05:17:40.716040Z",
     "shell.execute_reply": "2023-10-08T05:17:40.715202Z",
     "shell.execute_reply.started": "2023-10-08T05:17:35.171475Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing weeks for item 'HOUSEHOLD_1_159' in store 'CA_2': 219\n",
      "Weeks with missing prices for HOUSEHOLD_1_159 in CA_2: \n",
      "[11101, 11102, 11103, 11104, 11105, 11106, 11107, 11108, 11109, 11110, 11111, 11112, 11113, 11114, 11115, 11116, 11117, 11118, 11119, 11120, 11121, 11122, 11123, 11124, 11125, 11126, 11127, 11128, 11129, 11130, 11131, 11132, 11133, 11134, 11135, 11136, 11137, 11138, 11139, 11140, 11141, 11142, 11143, 11144, 11145, 11146, 11147, 11148, 11149, 11150, 11151, 11152, 11201, 11202, 11203, 11204, 11205, 11206, 11207, 11208, 11209, 11210, 11211, 11212, 11213, 11214, 11215, 11216, 11217, 11218, 11219, 11220, 11221, 11222, 11223, 11224, 11225, 11226, 11227, 11228, 11229, 11230, 11231, 11232, 11233, 11234, 11235, 11236, 11237, 11238, 11239, 11240, 11241, 11242, 11243, 11244, 11245, 11246, 11247, 11248, 11249, 11250, 11251, 11252, 11301, 11302, 11303, 11304, 11305, 11306, 11307, 11308, 11309, 11310, 11311, 11312, 11313, 11314, 11315, 11316, 11317, 11318, 11319, 11320, 11321, 11322, 11323, 11324, 11325, 11326, 11327, 11328, 11329, 11330, 11331, 11332, 11333, 11334, 11335, 11336, 11337, 11338, 11339, 11340, 11341, 11342, 11343, 11344, 11345, 11346, 11347, 11348, 11349, 11350, 11351, 11352, 11353, 11401, 11402, 11403, 11404, 11405, 11406, 11407, 11408, 11409, 11410, 11411, 11412, 11413, 11414, 11415, 11416, 11417, 11418, 11419, 11420, 11421, 11422, 11423, 11424, 11425, 11426, 11427, 11428, 11429, 11430, 11431, 11432, 11433, 11434, 11435, 11436, 11437, 11438, 11439, 11440, 11441, 11442, 11443, 11444, 11445, 11446, 11447, 11448, 11449, 11450, 11451, 11452, 11501, 11502, 11503, 11504, 11505, 11506, 11507, 11508, 11509, 11510]\n"
     ]
    }
   ],
   "source": [
    "# Filtering data for the item 'HOUSEHOLD_1_159' in store 'CA_2' which has missing prices\n",
    "filtered_data = merged_train[(merged_train['item_id'] == 'HOUSEHOLD_1_159') & \n",
    "                            (merged_train['store_id'] == 'CA_2') & \n",
    "                            (merged_train['sell_price'].isnull())]\n",
    "\n",
    "# Extract weeks with missing prices for this combination\n",
    "missing_weeks_for_combination = filtered_data['wm_yr_wk'].unique()\n",
    "\n",
    "print(f\"Number of missing weeks for item 'HOUSEHOLD_1_159' in store 'CA_2': {len(missing_weeks_for_combination)}\")\n",
    "print(f\"Weeks with missing prices for HOUSEHOLD_1_159 in CA_2: \\n{sorted(missing_weeks_for_combination)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0329d7b6-4030-482c-b0bd-d5aae723f1fc",
   "metadata": {},
   "source": [
    "#### Investigate the Missing Values:\n",
    "- If the percentage of missing values is significant, we should investigate further. For example, check if specific items or stores have more missing prices than others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ac7b5c76-60e5-4dd2-a496-474c3b40b22c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-08T05:17:40.717570Z",
     "iopub.status.busy": "2023-10-08T05:17:40.717160Z",
     "iopub.status.idle": "2023-10-08T05:17:46.178475Z",
     "shell.execute_reply": "2023-10-08T05:17:46.177749Z",
     "shell.execute_reply.started": "2023-10-08T05:17:40.717536Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of missing sell prices for 'HOUSEHOLD_1_159' in 'CA_2': 99.48%\n"
     ]
    }
   ],
   "source": [
    "subset_data = merged_train[(merged_train['item_id'] == 'HOUSEHOLD_1_159') & (merged_train['store_id'] == 'CA_2')]\n",
    "missing_percentage = (subset_data['sell_price'].isnull().sum() / len(subset_data)) * 100\n",
    "print(f\"Percentage of missing sell prices for 'HOUSEHOLD_1_159' in 'CA_2': {missing_percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "04f355b8-0893-4d6c-ab95-a66a86ea42c0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-08T05:17:46.179787Z",
     "iopub.status.busy": "2023-10-08T05:17:46.179458Z",
     "iopub.status.idle": "2023-10-08T05:18:00.165856Z",
     "shell.execute_reply": "2023-10-08T05:18:00.165225Z",
     "shell.execute_reply.started": "2023-10-08T05:17:46.179763Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              item_id store_id  sell_price\n",
      "0         FOODS_1_029     CA_4   97.669903\n",
      "1         FOODS_1_122     CA_1   90.873786\n",
      "2         FOODS_1_122     CA_2   90.873786\n",
      "3         FOODS_1_122     CA_3   90.873786\n",
      "4         FOODS_1_122     CA_4   90.873786\n",
      "...               ...      ...         ...\n",
      "1126  HOUSEHOLD_2_418     WI_2   92.686084\n",
      "1127  HOUSEHOLD_2_418     WI_3   93.139159\n",
      "1128  HOUSEHOLD_2_434     TX_3   92.686084\n",
      "1129  HOUSEHOLD_2_498     CA_4   92.686084\n",
      "1130  HOUSEHOLD_2_498     WI_3   93.592233\n",
      "\n",
      "[1131 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# Group by item_id and store_id\n",
    "grouped = merged_train.groupby(['item_id', 'store_id'])\n",
    "\n",
    "# Calculate the percentage of missing sell_price for each group\n",
    "missing_percentage = grouped['sell_price'].apply(lambda x: x.isnull().mean() * 100)\n",
    "\n",
    "# Filter out the combinations where the missing percentage is greater than 90%\n",
    "high_missing_combinations = missing_percentage[missing_percentage > 90].reset_index()\n",
    "\n",
    "print(high_missing_combinations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2c62de77-1ba4-4562-849c-877c4eb2fc36",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-08T05:18:00.167304Z",
     "iopub.status.busy": "2023-10-08T05:18:00.166756Z",
     "iopub.status.idle": "2023-10-08T05:18:06.913648Z",
     "shell.execute_reply": "2023-10-08T05:18:06.912967Z",
     "shell.execute_reply.started": "2023-10-08T05:18:00.167278Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.709412922269597\n"
     ]
    }
   ],
   "source": [
    "# Calculate the total number of unique item-store combinations in the merged_data\n",
    "total_item_store_combinations = merged_train[['item_id', 'store_id']].drop_duplicates().shape[0]\n",
    "\n",
    "# Calculate the percentage of problematic item-store combinations\n",
    "percentage_problematic = (1131 / total_item_store_combinations) * 100\n",
    "\n",
    "print(percentage_problematic)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9003d3-ab48-441d-8513-ea32006cf45e",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9defbd52-96e9-45b2-b6ea-06de04a85571",
   "metadata": {},
   "source": [
    "- Drop: If the missing data is random and constitutes a small portion of the dataset.\n",
    "- Impute: If the missing data follows a pattern or is a significant portion of the dataset.\n",
    "\n",
    "Action Plan:\n",
    "\n",
    "- Calculate the percentage of missing values for each feature.\n",
    "- Based on the percentage, decide whether to drop or impute.\n",
    "\n",
    "Let's first inspect the percentage of missing values in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f4ac8e-4cc7-42eb-a04b-95b7d5513234",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-09T22:32:36.890739Z",
     "iopub.status.busy": "2023-10-09T22:32:36.890456Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculate the percentage of missing values for each column\n",
    "missing_percentage = merged_train.isnull().mean() * 100\n",
    "\n",
    "# Display columns with missing values (if any)\n",
    "missing_percentage[missing_percentage > 0].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c3be7b-0793-462b-bf4c-a7eddc62d918",
   "metadata": {},
   "source": [
    "The sell_price is a crucial feature for predicting sales revenue. Dropping a quarter of our dataset might lead to a significant loss of information, which is undesirable. Instead, we should look to impute these missing values.\n",
    "\n",
    "There are various strategies to impute missing values:\n",
    "\n",
    "- Mean/Median Imputation: Replace missing values with the mean or median of the column. This approach is suitable for features that have a normal distribution or when the missing data is completely random.\n",
    "- Mode Imputation: Replace missing values with the most frequent value. This is more applicable for categorical data.\n",
    "- Time Series Imputation: For time series data, one can use methods like forward fill or backward fill."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054416b2-1179-414c-b4a6-aeeea1135799",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Compute the median sell price for each store-item combination\n",
    "medians = merged_train.groupby(['store_id', 'item_id'])['sell_price'].transform('median')\n",
    "\n",
    "# Fill missing values in 'sell_price' with the corresponding median values\n",
    "merged_train['sell_price'].fillna(medians, inplace=True)\n",
    "\n",
    "# Verify if there are any missing values left\n",
    "missing_after_impute = merged_train['sell_price'].isnull().sum()\n",
    "print(f\"Number of missing values after imputation: {missing_after_impute}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6801ba3-8b83-4dd0-8ab6-0c0011f97f03",
   "metadata": {},
   "source": [
    "We can see a significant reduction from the original number of missing values, but there are still some left. The remaining missing values could be due to items that never had a sale price listed in the given data. For such cases, we have a few options:\n",
    "\n",
    "- Drop the rows with missing sell prices: This is the simplest method but may result in loss of some data.\n",
    "- Fill with a global median or mean: Calculate the median or mean of the sell_price column and fill the remaining missing values with it. This might introduce some bias, but it is a quick way to handle missing values.\n",
    "- Assign a default value: Sometimes, assigning a default value (like 0) might be an option, but in this case, it might not be logical as a sell price of 0 doesn't make sense.\n",
    "\n",
    "Given the nature of our dataset and the task, we want to use the second option (filling with a global median or mean). This ensures we don't lose any rows of data, and while there might be some bias introduced, it's likely a reasonable estimate for items with no historical sell price data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c245ad-90a4-4639-bb31-7ba66dd3716e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculate the global median of the 'sell_price' column\n",
    "sell_price_median = merged_train['sell_price'].median()\n",
    "\n",
    "# Fill the remaining missing values with the calculated median\n",
    "merged_train['sell_price'].fillna(sell_price_median, inplace=True)\n",
    "\n",
    "# Check again the number of missing values\n",
    "missing_values_after = merged_train['sell_price'].isnull().sum()\n",
    "\n",
    "print(f\"Number of missing values after imputation: {missing_values_after}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58357331-52e5-4ba4-b8cc-de1c1791f24e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "merged_train['event_name'].fillna(\"NoEvent\", inplace=True)\n",
    "merged_train['event_type'].fillna(\"NoEvent\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6e9b17-5c56-43ce-b697-e59c9bb1bbc4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "merged_train.tail(500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29338827-55dc-45c3-be22-b7114388d690",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-08T05:57:57.040705Z",
     "iopub.status.busy": "2023-10-08T05:57:57.040333Z",
     "iopub.status.idle": "2023-10-08T05:57:57.043770Z",
     "shell.execute_reply": "2023-10-08T05:57:57.043160Z",
     "shell.execute_reply.started": "2023-10-08T05:57:57.040680Z"
    },
    "tags": []
   },
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94e690a-739a-4190-b7d7-7d59ad76b123",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data = merged_train.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84de09ac-79b1-4bf2-b08d-dcaa2de580a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculating Sales Revenue for training purpose\n",
    "train_data['sales_revenue'] = train_data['sales'] * train_data['sell_price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6248cb68-4437-4ea7-9713-d84cbaa24c7c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5321415-0980-4c61-91d6-ff5a31670ac2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from features.build_features import create_lagged_and_rolling_features, create_trend_and_special_day_features\n",
    "\n",
    "train_data = create_lagged_and_rolling_features(train_data)\n",
    "train_data = create_trend_and_special_day_features(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6330b8-80c6-45b8-96ef-4826deec6290",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "train_data.tail(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e56c4a6a-21bf-4c30-b2ad-20214119cf06",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-09T15:14:04.763612Z",
     "iopub.status.busy": "2023-10-09T15:14:04.763244Z",
     "iopub.status.idle": "2023-10-09T15:14:15.247027Z",
     "shell.execute_reply": "2023-10-09T15:14:15.246414Z",
     "shell.execute_reply.started": "2023-10-09T15:14:04.763593Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data exported to recent_data.parquet\n"
     ]
    }
   ],
   "source": [
    "# Exporting past 90 days data to use it for our deployment usage\n",
    "\n",
    "from data.export import export_to_parquet\n",
    "export_to_parquet(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942b4143-2d28-4687-9398-ef27c34bfc4d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from features.label_encode import label_encode_data\n",
    "\n",
    "# Columns to label encode\n",
    "cat_columns = ['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id', 'event_name', 'event_type']\n",
    "\n",
    "train_data, encoders_dict = label_encode_data(train_data, cat_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fc24cd01-1cae-46dc-99d6-ea0da5546c28",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-09T15:26:42.236717Z",
     "iopub.status.busy": "2023-10-09T15:26:42.236395Z",
     "iopub.status.idle": "2023-10-09T15:26:42.249194Z",
     "shell.execute_reply": "2023-10-09T15:26:42.248558Z",
     "shell.execute_reply.started": "2023-10-09T15:26:42.236693Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['encoders_dict.joblib']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from joblib import dump\n",
    "dump(encoders_dict, 'encoders_dict.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1fa059-ec82-45eb-9121-aea30a2fd387",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7eee19-2102-4aa2-aba9-8ddc38a5770a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from models.train_model import train_val_split\n",
    "\n",
    "train_df, val_df = train_val_split(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7ac2b4-9034-4d6c-8d1e-d0ed12b74b85",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "# Delete the unused dataframes\n",
    "del df1, df2, df3, df4, df5, df5_melted, df1_filtered, merged_data, missing_percentage, train_data\n",
    "\n",
    "# Force garbage collection\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c85a5d38-570a-4543-a5b4-97b34759953d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-09T15:33:30.106346Z",
     "iopub.status.busy": "2023-10-09T15:33:30.105682Z",
     "iopub.status.idle": "2023-10-09T15:33:40.246444Z",
     "shell.execute_reply": "2023-10-09T15:33:40.245806Z",
     "shell.execute_reply.started": "2023-10-09T15:33:30.106320Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample_df = train_df.sample(frac=0.5)  # Takes a 50% random sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f8945d-ebdd-4e62-8c48-f4e5e2668a96",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-09T15:33:54.141477Z",
     "iopub.status.busy": "2023-10-09T15:33:54.141083Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/saturncloud/envs/saturn/lib/python3.9/site-packages/xgboost/data.py:262: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n",
      "/opt/saturncloud/envs/saturn/lib/python3.9/site-packages/xgboost/data.py:262: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-rmse:9.23413\teval-rmse:11.10508\n",
      "[10]\ttrain-rmse:7.73214\teval-rmse:9.47759\n",
      "[20]\ttrain-rmse:6.95896\teval-rmse:8.61812\n",
      "[30]\ttrain-rmse:6.47771\teval-rmse:7.84849\n",
      "[40]\ttrain-rmse:6.25703\teval-rmse:7.46436\n",
      "[50]\ttrain-rmse:6.11586\teval-rmse:7.18638\n"
     ]
    }
   ],
   "source": [
    "from models.train_model import train_xgb_model\n",
    "\n",
    "# Features and target variable\n",
    "features = [col for col in sample_df.columns if col not in ['sales_revenue', 'date', 'sales']]\n",
    "target = 'sales_revenue'\n",
    "\n",
    "model = train_xgb_model(sample_df, val_df, features, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e110b8eb-82f1-4364-a25c-e081741ca766",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from joblib import dump\n",
    "model_filename = 'xgb_model_sample.joblib'\n",
    "dump(model, model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "36089d8a-3722-4b82-811e-c355ce678bcb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-09T07:52:01.294756Z",
     "iopub.status.busy": "2023-10-09T07:52:01.294370Z",
     "iopub.status.idle": "2023-10-09T07:52:01.299783Z",
     "shell.execute_reply": "2023-10-09T07:52:01.299166Z",
     "shell.execute_reply.started": "2023-10-09T07:52:01.294731Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'models.train_model' from '/home/jovyan/workspace/ml_prediction_forecasting/notebooks/../src/models/train_model.py'>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from importlib import reload\n",
    "import models.train_model\n",
    "reload(models.train_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258fb53e-7a10-46a7-b249-0e86326c3374",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8800c284-8397-44c5-817e-cce2cf6f384f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "\n",
    "# Load the model\n",
    "from joblib import load\n",
    "loaded_model = load('xgb_model_sample.joblib')\n",
    "\n",
    "# Features and target variable\n",
    "features = [col for col in train_df.columns if col not in ['sales_revenue', 'date', 'sales']]\n",
    "target = 'sales_revenue'\n",
    "\n",
    "# Extract features and target from train and validation datasets\n",
    "train_features = train_df[features]\n",
    "train_labels = train_df[target]\n",
    "\n",
    "val_features = val_df[features]\n",
    "val_labels = val_df[target]\n",
    "\n",
    "# Convert features to DMatrix format for prediction\n",
    "train_data_matrix = xgb.DMatrix(train_features)\n",
    "val_data_matrix = xgb.DMatrix(val_features)\n",
    "\n",
    "# Use the loaded model to predict the sales revenue\n",
    "train_predictions = loaded_model.predict(train_data_matrix)\n",
    "val_predictions = loaded_model.predict(val_data_matrix)\n",
    "\n",
    "# Compute RMSE for train and validation predictions\n",
    "train_rmse = np.sqrt(mean_squared_error(train_labels, train_predictions))\n",
    "val_rmse = np.sqrt(mean_squared_error(val_labels, val_predictions))\n",
    "\n",
    "print(f\"Train RMSE: {train_rmse}\")\n",
    "print(f\"Validation RMSE: {val_rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9bc6438-d0ba-43cb-a1e0-19e9c8aa1c89",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Predict on validation set\n",
    "val_predictions = loaded_model.predict(xgb.DMatrix(val_df[features]))\n",
    "\n",
    "# Scatter plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(val_df[target], val_predictions, alpha=0.3)\n",
    "plt.plot([val_df[target].min(), val_df[target].max()], [val_df[target].min(), val_df[target].max()], 'k--', lw=3)\n",
    "plt.xlabel('Actual')\n",
    "plt.ylabel('Predicted')\n",
    "plt.title('Actual vs. Predicted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "befce167-c7dd-4e76-91c0-af571d33047a",
   "metadata": {},
   "source": [
    "Actual vs. Predicted Plot: This plot will show the predicted values against the actual values. If the model is perfect, all the points should lie along a 45-degree line (y=x line)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7fa991-1628-4937-be66-51eb255f23f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "residuals = val_df[target] - val_predictions\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(val_predictions, residuals, alpha=0.3)\n",
    "plt.plot([val_predictions.min(), val_predictions.max()], [0, 0], 'k--', lw=3)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title('Residuals vs. Predicted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17fa7b40-c4b5-4f8f-9403-d5a99ef7d8f6",
   "metadata": {},
   "source": [
    "Residuals Plot: Plotting residuals (difference between actual and predicted values) can help identify any patterns that the model might not be capturing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c42a050-6f25-46a2-9825-c2c98fc21fe8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Extract feature importances\n",
    "feature_importances = loaded_model.get_score(importance_type='weight')\n",
    "\n",
    "# Sort features by importance\n",
    "sorted_idx = sorted(feature_importances, key=feature_importances.get, reverse=True)\n",
    "\n",
    "plt.figure(figsize=(10, 12))\n",
    "plt.barh(range(len(sorted_idx)), [feature_importances[i] for i in sorted_idx], align='center')\n",
    "plt.yticks(range(len(sorted_idx)), sorted_idx)\n",
    "plt.xlabel('Importance')\n",
    "plt.title('Feature Importances')\n",
    "plt.gca().invert_yaxis()  # Highest importance at the top\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3adf8ba5-adb1-4004-9944-aa5743653be5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-09T14:28:13.539324Z",
     "iopub.status.busy": "2023-10-09T14:28:13.538933Z",
     "iopub.status.idle": "2023-10-09T14:28:13.542943Z",
     "shell.execute_reply": "2023-10-09T14:28:13.542378Z",
     "shell.execute_reply.started": "2023-10-09T14:28:13.539298Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4.2\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "print(pd.__version__)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "saturn (Python 3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
